{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705efc16",
   "metadata": {},
   "source": [
    "# 4 Modeling, feature selection, training, validaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d098497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Modeling, feature selection, training, validaton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c80ac1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7140, 79)\n",
      "['Date', 'Competition', 'Opponent', 'Player', '#', 'Nation', 'Pos', 'Age', 'Min', ' Gls', ' Ast', ' PK', ' PKatt', ' Sh', ' SoT', ' CrdY', ' CrdR', ' Int', 'Match URL', 'Season', ' Touches', ' Tkl', ' Blocks', 'Expected xG', 'Expected npxG', 'Expected xAG', 'SCA', 'GCA', 'Passes Cmp', 'Passes Att', 'Passes Cmp%', 'Passes PrgP', 'Carries Carries', 'Carries PrgC', 'Take-Ons Att', 'Take-Ons Succ', 'Tackles Tkl', 'Tackles TklW', 'Tackles Def 3rd', 'Tackles Mid 3rd', 'Tackles Att 3rd', 'Challenges Tkl', 'Challenges Att', 'Challenges Tkl%', 'Challenges Lost', 'Blocks Blocks', 'Blocks Sh', 'Blocks Pass', 'Int', 'Tkl+Int', 'Clr', 'Err', 'Total Cmp', 'Total Att', 'Total Cmp%', 'Total TotDist', 'Total PrgDist', 'Short Cmp', 'Short Att', 'Short Cmp%', 'Medium Cmp', 'Medium Att', 'Medium Cmp%', 'Long Cmp', 'Long Att', 'Long Cmp%', 'Ast', 'xAG', 'xA', 'KP', '3-Jan', 'PPA', 'CrsPA', 'PrgP', 'SCA SCA', 'SCA GCA', ' 1/3', 'Position_Group', 'Rebalanced_Score']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your rebalanced dataset\n",
    "path = '/Users/home/Capstone/ADS599_Capstone/Main Notebook/Code Library Folder/02_Feature_Engineering/outputs/real_madrid_rebalanced_scores.csv'\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(path)\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78323ecf",
   "metadata": {},
   "source": [
    "## 2. Add Weekly ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f185553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV with Week column saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/5yylt10156q8bq2drpk08r7w0000gn/T/ipykernel_24954/1887114791.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Date'] = pd.to_datetime(df['Date'])\n"
     ]
    }
   ],
   "source": [
    "# After adding the Week column\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Week'] = df['Date'].dt.isocalendar().week\n",
    "else:\n",
    "    df['Week'] = (df.index // 10) + 1\n",
    "\n",
    "# Save the updated DataFrame back to the same file\n",
    "df.to_csv(path, index=False)\n",
    "print(\"Updated CSV with Week column saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2587fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " POSITION-SPECIFIC ML TRAINING\n",
      "================================================================================\n",
      "Dataset shape: (7140, 80)\n",
      "Columns: ['Date', 'Competition', 'Opponent', 'Player', '#', 'Nation', 'Pos', 'Age', 'Min', ' Gls', ' Ast', ' PK', ' PKatt', ' Sh', ' SoT', ' CrdY', ' CrdR', ' Int', 'Match URL', 'Season', ' Touches', ' Tkl', ' Blocks', 'Expected xG', 'Expected npxG', 'Expected xAG', 'SCA', 'GCA', 'Passes Cmp', 'Passes Att', 'Passes Cmp%', 'Passes PrgP', 'Carries Carries', 'Carries PrgC', 'Take-Ons Att', 'Take-Ons Succ', 'Tackles Tkl', 'Tackles TklW', 'Tackles Def 3rd', 'Tackles Mid 3rd', 'Tackles Att 3rd', 'Challenges Tkl', 'Challenges Att', 'Challenges Tkl%', 'Challenges Lost', 'Blocks Blocks', 'Blocks Sh', 'Blocks Pass', 'Int', 'Tkl+Int', 'Clr', 'Err', 'Total Cmp', 'Total Att', 'Total Cmp%', 'Total TotDist', 'Total PrgDist', 'Short Cmp', 'Short Att', 'Short Cmp%', 'Medium Cmp', 'Medium Att', 'Medium Cmp%', 'Long Cmp', 'Long Att', 'Long Cmp%', 'Ast', 'xAG', 'xA', 'KP', '3-Jan', 'PPA', 'CrsPA', 'PrgP', 'SCA SCA', 'SCA GCA', ' 1/3', 'Position_Group', 'Rebalanced_Score', 'Week']\n",
      "Week range: 1 - 53\n",
      "Position groups: ['Forward' 'Midfield' 'Defense' 'Goalkeeper']\n",
      "\n",
      "Forward:\n",
      "  Available core metrics (6): [' Gls', 'Ast', ' SoT', 'Expected xG', 'Expected xAG', 'Take-Ons Succ']\n",
      "  Available secondary metrics (1): ['Min']\n",
      "\n",
      "Midfield:\n",
      "  Available core metrics (6): ['Passes Cmp%', 'KP', ' Tkl', 'Carries PrgC', 'Passes PrgP', ' Touches']\n",
      "  Available secondary metrics (1): ['Min']\n",
      "\n",
      "Defense:\n",
      "  Available core metrics (8): ['Int', ' Blocks', 'Clr', 'Tackles TklW', 'Tackles Def 3rd', 'Tackles Mid 3rd', 'Blocks Sh', 'Blocks Pass']\n",
      "  Available secondary metrics (1): ['Min']\n",
      "\n",
      "Goalkeeper:\n",
      "  Available core metrics (7): ['Total Cmp%', 'Err', 'Total PrgDist', 'Short Cmp%', 'Medium Cmp%', 'Total Cmp', 'Short Att']\n",
      "  Available secondary metrics (1): ['Min']\n",
      "\n",
      "============================================================\n",
      "CREATING DATASETS FOR FORWARD\n",
      "============================================================\n",
      "Total Forward observations with valid scores: 1695\n",
      "Unique players: 31\n",
      "Metrics to use: 7\n",
      "Available metrics (7): [' Gls', 'Ast', ' SoT', 'Expected xG', 'Expected xAG', 'Take-Ons Succ', 'Min']\n",
      "Training weeks: 1 - 51\n",
      "Testing weeks: 52 - 53\n",
      "Train size: 1669, Test size: 26\n",
      "\n",
      "============================================================\n",
      "CREATING DATASETS FOR MIDFIELD\n",
      "============================================================\n",
      "Total Midfield observations with valid scores: 1823\n",
      "Unique players: 34\n",
      "Metrics to use: 7\n",
      "Available metrics (7): ['Passes Cmp%', 'KP', ' Tkl', 'Carries PrgC', 'Passes PrgP', ' Touches', 'Min']\n",
      "Training weeks: 1 - 51\n",
      "Testing weeks: 52 - 53\n",
      "Train size: 1801, Test size: 22\n",
      "\n",
      "============================================================\n",
      "CREATING DATASETS FOR DEFENSE\n",
      "============================================================\n",
      "Total Defense observations with valid scores: 1823\n",
      "Unique players: 28\n",
      "Metrics to use: 9\n",
      "Available metrics (9): ['Int', ' Blocks', 'Clr', 'Tackles TklW', 'Tackles Def 3rd', 'Tackles Mid 3rd', 'Blocks Sh', 'Blocks Pass', 'Min']\n",
      "Training weeks: 1 - 51\n",
      "Testing weeks: 52 - 53\n",
      "Train size: 1801, Test size: 22\n",
      "\n",
      "============================================================\n",
      "CREATING DATASETS FOR GOALKEEPER\n",
      "============================================================\n",
      "Total Goalkeeper observations with valid scores: 498\n",
      "Unique players: 6\n",
      "Metrics to use: 8\n",
      "Available metrics (8): ['Total Cmp%', 'Err', 'Total PrgDist', 'Short Cmp%', 'Medium Cmp%', 'Total Cmp', 'Short Att', 'Min']\n",
      "Training weeks: 1 - 51\n",
      "Testing weeks: 52 - 53\n",
      "Train size: 491, Test size: 7\n",
      "\n",
      "Successfully created datasets for 4 positions: ['Forward', 'Midfield', 'Defense', 'Goalkeeper']\n",
      "\n",
      "TRAINING MODEL FOR FORWARD\n",
      "--------------------------------------------------\n",
      "MODEL PERFORMANCE:\n",
      "  Training R²: 0.993\n",
      "  Testing R²:  0.969\n",
      "  Training MAE: 0.445\n",
      "  Testing MAE:  0.790\n",
      "  Training RMSE: 0.671\n",
      "  Testing RMSE:  1.258\n",
      "\n",
      "TOP 5 IMPORTANT METRICS:\n",
      "   Gls: 0.705\n",
      "  Expected xG: 0.113\n",
      "  Ast: 0.107\n",
      "  Min: 0.041\n",
      "   SoT: 0.032\n",
      "\n",
      "TRAINING MODEL FOR MIDFIELD\n",
      "--------------------------------------------------\n",
      "MODEL PERFORMANCE:\n",
      "  Training R²: 0.918\n",
      "  Testing R²:  0.804\n",
      "  Training MAE: 0.729\n",
      "  Testing MAE:  1.039\n",
      "  Training RMSE: 1.152\n",
      "  Testing RMSE:  1.620\n",
      "\n",
      "TOP 5 IMPORTANT METRICS:\n",
      "  KP: 0.583\n",
      "  Passes Cmp%: 0.153\n",
      "  Passes PrgP: 0.079\n",
      "   Tkl: 0.075\n",
      "  Min: 0.062\n",
      "\n",
      "TRAINING MODEL FOR DEFENSE\n",
      "--------------------------------------------------\n",
      "MODEL PERFORMANCE:\n",
      "  Training R²: 0.993\n",
      "  Testing R²:  0.995\n",
      "  Training MAE: 0.173\n",
      "  Testing MAE:  0.141\n",
      "  Training RMSE: 0.354\n",
      "  Testing RMSE:  0.248\n",
      "\n",
      "TOP 5 IMPORTANT METRICS:\n",
      "   Blocks: 0.446\n",
      "  Int: 0.260\n",
      "  Tackles TklW: 0.188\n",
      "  Min: 0.054\n",
      "  Clr: 0.050\n",
      "\n",
      "TRAINING MODEL FOR GOALKEEPER\n",
      "--------------------------------------------------\n",
      "MODEL PERFORMANCE:\n",
      "  Training R²: 0.994\n",
      "  Testing R²:  0.719\n",
      "  Training MAE: 0.044\n",
      "  Testing MAE:  0.488\n",
      "  Training RMSE: 0.172\n",
      "  Testing RMSE:  0.964\n",
      "\n",
      "TOP 5 IMPORTANT METRICS:\n",
      "  Total Cmp%: 0.871\n",
      "  Err: 0.049\n",
      "  Total PrgDist: 0.033\n",
      "  Medium Cmp%: 0.028\n",
      "  Total Cmp: 0.018\n",
      "\n",
      "All position-specific models trained successfully!\n",
      "Trained models for: ['Forward', 'Midfield', 'Defense', 'Goalkeeper']\n",
      "\n",
      "================================================================================\n",
      "POSITION-SPECIFIC MODEL SUMMARY\n",
      "================================================================================\n",
      "Position     Test R²    Test MAE   Top Metric           Importance\n",
      "----------------------------------------------------------------------\n",
      "Forward      0.969      0.790       Gls                 0.705     \n",
      "Midfield     0.804      1.039      KP                   0.583     \n",
      "Defense      0.995      0.141       Blocks              0.446     \n",
      "Goalkeeper   0.719      0.488      Total Cmp%           0.871     \n",
      "\n",
      "Position-specific ML training complete!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" POSITION-SPECIFIC ML TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ==========================================\n",
    "# DATA LOADING AND PREPARATION\n",
    "# ==========================================\n",
    "# Path to your rebalanced dataset\n",
    "path = '/Users/home/Capstone/ADS599_Capstone/Main Notebook/Code Library Folder/02_Feature_Engineering/outputs/real_madrid_rebalanced_scores.csv'\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(path)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Add Week column if it doesn't exist\n",
    "if 'Week' not in df.columns:\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df['Week'] = df['Date'].dt.isocalendar().week\n",
    "    else:\n",
    "        # Simulated weeks for demonstration\n",
    "        df['Week'] = (df.index // 10) + 1\n",
    "    \n",
    "    # Save back to file\n",
    "    df.to_csv(path, index=False)\n",
    "    print(\"Week column added and saved!\")\n",
    "\n",
    "print(f\"Week range: {df['Week'].min()} - {df['Week'].max()}\")\n",
    "print(f\"Position groups: {df['Position_Group'].unique()}\")\n",
    "\n",
    "# ==========================================\n",
    "# POSITION-SPECIFIC METRIC DEFINITIONS\n",
    "# ==========================================\n",
    "\n",
    "# Updated position metrics based on multicollinearity-free variables (REVISED FINAL LIST)\n",
    "position_metrics = {\n",
    "    'Forward': {\n",
    "        'core_metrics': ['Gls', 'Ast', 'SoT', 'Expected xG', 'Expected xAG', 'Take-Ons Succ'],\n",
    "        'secondary_metrics': ['Min'],\n",
    "        'description': 'Goal scoring and creativity metrics (multicollinearity-free)'\n",
    "    },\n",
    "    'Midfield': {\n",
    "        'core_metrics': ['Passes Cmp%', 'KP', 'Tkl', 'Carries PrgC', 'Passes PrgP', 'Touches'],\n",
    "        'secondary_metrics': ['Min'],\n",
    "        'description': 'Passing, creativity, and defensive contribution metrics (multicollinearity-free)'\n",
    "    },\n",
    "    'Defense': {\n",
    "        'core_metrics': ['Int', 'Blocks', 'Clr', 'Tackles TklW', 'Tackles Def 3rd', 'Tackles Mid 3rd', 'Blocks Sh', 'Blocks Pass'],\n",
    "        'secondary_metrics': ['Min'],\n",
    "        'description': 'Defensive actions and positioning metrics (multicollinearity-free)'\n",
    "    },\n",
    "    'Goalkeeper': {\n",
    "        'core_metrics': ['Total Cmp%', 'Err', 'Total PrgDist', 'Short Cmp%', 'Medium Cmp%', 'Total Cmp', 'Short Att'],\n",
    "        'secondary_metrics': ['Min'],\n",
    "        'description': 'Distribution accuracy and consistency metrics (multicollinearity-free)'\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_available_metrics(df, position_metrics_dict):\n",
    "    \"\"\"Get metrics that actually exist in the dataset\"\"\"\n",
    "    available_metrics = {}\n",
    "    for position, metrics in position_metrics_dict.items():\n",
    "        # Check both core and secondary metrics\n",
    "        available_core = []\n",
    "        for m in metrics['core_metrics']:\n",
    "            if m in df.columns:\n",
    "                available_core.append(m)\n",
    "            elif f' {m}' in df.columns:  # Handle space prefix\n",
    "                available_core.append(f' {m}')\n",
    "        \n",
    "        available_secondary = []\n",
    "        for m in metrics['secondary_metrics']:\n",
    "            if m in df.columns:\n",
    "                available_secondary.append(m)\n",
    "            elif f' {m}' in df.columns:  # Handle space prefix\n",
    "                available_secondary.append(f' {m}')\n",
    "        \n",
    "        available_metrics[position] = {\n",
    "            'core_metrics': available_core,\n",
    "            'secondary_metrics': available_secondary,\n",
    "            'all_metrics': available_core + available_secondary,\n",
    "            'description': metrics['description']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{position}:\")\n",
    "        print(f\"  Available core metrics ({len(available_core)}): {available_core}\")\n",
    "        print(f\"  Available secondary metrics ({len(available_secondary)}): {available_secondary}\")\n",
    "    \n",
    "    return available_metrics\n",
    "\n",
    "available_metrics = get_available_metrics(df, position_metrics)\n",
    "\n",
    "# ==========================================\n",
    "# POSITION-SPECIFIC TRAIN/TEST SPLIT\n",
    "# ==========================================\n",
    "\n",
    "def create_position_datasets(df, position, metrics_list, test_weeks=2):\n",
    "    \"\"\"\n",
    "    Create position-specific train/test datasets\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CREATING DATASETS FOR {position.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Filter by position and remove rows with NaN in Rebalanced_Score\n",
    "    position_data = df[(df['Position_Group'] == position) & (df['Rebalanced_Score'].notna())].copy()\n",
    "    \n",
    "    if len(position_data) == 0:\n",
    "        print(f\"No data found for {position} with valid Rebalanced_Score\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Total {position} observations with valid scores: {len(position_data)}\")\n",
    "    print(f\"Unique players: {position_data['Player'].nunique()}\")\n",
    "    print(f\"Metrics to use: {len(metrics_list)}\")\n",
    "    \n",
    "    # Check which metrics are available\n",
    "    available_metrics_for_pos = [m for m in metrics_list if m in position_data.columns]\n",
    "    missing_metrics = [m for m in metrics_list if m not in position_data.columns]\n",
    "    \n",
    "    print(f\"Available metrics ({len(available_metrics_for_pos)}): {available_metrics_for_pos}\")\n",
    "    if missing_metrics:\n",
    "        print(f\"Missing metrics ({len(missing_metrics)}): {missing_metrics}\")\n",
    "    \n",
    "    if len(available_metrics_for_pos) < 3:\n",
    "        print(f\"Insufficient metrics for {position} (need at least 3)\")\n",
    "        return None\n",
    "    \n",
    "    # Time-based split (latest weeks for testing)\n",
    "    latest_week = position_data['Week'].max()\n",
    "    test_start_week = latest_week - test_weeks + 1\n",
    "    \n",
    "    train_data = position_data[position_data['Week'] < test_start_week]\n",
    "    test_data = position_data[position_data['Week'] >= test_start_week]\n",
    "    \n",
    "    print(f\"Training weeks: {train_data['Week'].min()} - {train_data['Week'].max()}\")\n",
    "    print(f\"Testing weeks: {test_data['Week'].min()} - {test_data['Week'].max()}\")\n",
    "    print(f\"Train size: {len(train_data)}, Test size: {len(test_data)}\")\n",
    "    \n",
    "    if len(train_data) < 10:\n",
    "        print(f\"Insufficient training data for {position} (need at least 10)\")\n",
    "        return None\n",
    "    \n",
    "    if len(test_data) < 1:\n",
    "        print(f\"Insufficient test data for {position} (need at least 1)\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X_train = train_data[available_metrics_for_pos].fillna(0)\n",
    "    y_train = train_data['Rebalanced_Score']\n",
    "    \n",
    "    X_test = test_data[available_metrics_for_pos].fillna(0)\n",
    "    y_test = test_data['Rebalanced_Score']\n",
    "    \n",
    "    # Store additional info\n",
    "    train_players = train_data['Player'].tolist()\n",
    "    test_players = test_data['Player'].tolist()\n",
    "    \n",
    "    return {\n",
    "        'position': position,\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'train_players': train_players,\n",
    "        'test_players': test_players,\n",
    "        'metrics_used': available_metrics_for_pos,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data\n",
    "    }\n",
    "\n",
    "# ==========================================\n",
    "# CREATE DATASETS FOR ALL POSITIONS\n",
    "# ==========================================\n",
    "\n",
    "position_datasets = {}\n",
    "\n",
    "for position in ['Forward', 'Midfield', 'Defense', 'Goalkeeper']:\n",
    "    if position in available_metrics:\n",
    "        # Use all available metrics for this position\n",
    "        metrics_to_use = available_metrics[position]['all_metrics']\n",
    "        \n",
    "        dataset = create_position_datasets(df, position, metrics_to_use)\n",
    "        \n",
    "        if dataset is not None:\n",
    "            position_datasets[position] = dataset\n",
    "        else:\n",
    "            print(f\"Skipping {position} due to insufficient data\")\n",
    "\n",
    "print(f\"\\nSuccessfully created datasets for {len(position_datasets)} positions: {list(position_datasets.keys())}\")\n",
    "\n",
    "# ==========================================\n",
    "# TRAIN POSITION-SPECIFIC MODELS\n",
    "# ==========================================\n",
    "\n",
    "def train_position_model(dataset_info):\n",
    "    \"\"\"\n",
    "    Train Random Forest model for specific position\n",
    "    \"\"\"\n",
    "    position = dataset_info['position']\n",
    "    X_train = dataset_info['X_train']\n",
    "    y_train = dataset_info['y_train']\n",
    "    X_test = dataset_info['X_test']\n",
    "    y_test = dataset_info['y_test']\n",
    "    \n",
    "    print(f\"\\nTRAINING MODEL FOR {position.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    print(f\"MODEL PERFORMANCE:\")\n",
    "    print(f\"  Training R²: {train_r2:.3f}\")\n",
    "    print(f\"  Testing R²:  {test_r2:.3f}\")\n",
    "    print(f\"  Training MAE: {train_mae:.3f}\")\n",
    "    print(f\"  Testing MAE:  {test_mae:.3f}\")\n",
    "    print(f\"  Training RMSE: {train_rmse:.3f}\")\n",
    "    print(f\"  Testing RMSE:  {test_rmse:.3f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'metric': X_train.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTOP 5 IMPORTANT METRICS:\")\n",
    "    for _, row in feature_importance.head().iterrows():\n",
    "        print(f\"  {row['metric']}: {row['importance']:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'feature_importance': feature_importance,\n",
    "        'metrics': {\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'train_rmse': train_rmse,\n",
    "            'test_rmse': test_rmse\n",
    "        },\n",
    "        'predictions': {\n",
    "            'y_train_pred': y_train_pred,\n",
    "            'y_test_pred': y_test_pred\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Train models for each position\n",
    "position_models = {}\n",
    "\n",
    "for position, dataset in position_datasets.items():\n",
    "    model_info = train_position_model(dataset)\n",
    "    position_models[position] = model_info\n",
    "\n",
    "print(f\"\\nAll position-specific models trained successfully!\")\n",
    "print(f\"Trained models for: {list(position_models.keys())}\")\n",
    "\n",
    "# ==========================================\n",
    "# MODEL SUMMARY\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POSITION-SPECIFIC MODEL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"{'Position':<12} {'Test R²':<10} {'Test MAE':<10} {'Top Metric':<20} {'Importance':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for position, model_info in position_models.items():\n",
    "    test_r2 = model_info['metrics']['test_r2']\n",
    "    test_mae = model_info['metrics']['test_mae']\n",
    "    top_metric = model_info['feature_importance'].iloc[0]['metric']\n",
    "    top_importance = model_info['feature_importance'].iloc[0]['importance']\n",
    "    \n",
    "    print(f\"{position:<12} {test_r2:<10.3f} {test_mae:<10.3f} {top_metric:<20} {top_importance:<10.3f}\")\n",
    "\n",
    "print(\"\\nPosition-specific ML training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5fbb891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully: (7140, 80)\n"
     ]
    }
   ],
   "source": [
    "#XGBOOST\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load data\n",
    "path = '/Users/home/Capstone/ADS599_Capstone/Main Notebook/Code Library Folder/02_Feature_Engineering/outputs/real_madrid_rebalanced_scores.csv'\n",
    "df = pd.read_csv(path)\n",
    "print(f\"Data loaded successfully: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ea02b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOOST TRAINING WITH RAW MATCH STATISTICS\n",
      "============================================================\n",
      "Dataset shape: (7140, 80)\n",
      "Available columns: ['Date', 'Competition', 'Opponent', 'Player', '#', 'Nation', 'Pos', 'Age', 'Min', ' Gls', ' Ast', ' PK', ' PKatt', ' Sh', ' SoT', ' CrdY', ' CrdR', ' Int', 'Match URL', 'Season', ' Touches', ' Tkl', ' Blocks', 'Expected xG', 'Expected npxG', 'Expected xAG', 'SCA', 'GCA', 'Passes Cmp', 'Passes Att', 'Passes Cmp%', 'Passes PrgP', 'Carries Carries', 'Carries PrgC', 'Take-Ons Att', 'Take-Ons Succ', 'Tackles Tkl', 'Tackles TklW', 'Tackles Def 3rd', 'Tackles Mid 3rd', 'Tackles Att 3rd', 'Challenges Tkl', 'Challenges Att', 'Challenges Tkl%', 'Challenges Lost', 'Blocks Blocks', 'Blocks Sh', 'Blocks Pass', 'Int', 'Tkl+Int', 'Clr', 'Err', 'Total Cmp', 'Total Att', 'Total Cmp%', 'Total TotDist', 'Total PrgDist', 'Short Cmp', 'Short Att', 'Short Cmp%', 'Medium Cmp', 'Medium Att', 'Medium Cmp%', 'Long Cmp', 'Long Att', 'Long Cmp%', 'Ast', 'xAG', 'xA', 'KP', '3-Jan', 'PPA', 'CrsPA', 'PrgP', 'SCA SCA', 'SCA GCA', ' 1/3', 'Position_Group', 'Rebalanced_Score', 'Week']\n",
      "\n",
      "Available features (13): ['Min', ' Gls', ' Ast', ' SoT', ' Tkl', ' Int', ' Blocks', 'Passes Cmp%', 'Expected xG', 'Expected xAG', 'Take-Ons Succ', 'Carries PrgC', 'Passes PrgP']\n",
      "Missing features (3): [' KP', ' Clr', 'Touches']\n",
      "\n",
      "==================================================\n",
      "TRAINING COMBINED MODEL\n",
      "==================================================\n",
      "Clean dataset: 5839 observations\n",
      "Week range: 1 - 53\n",
      "Training weeks: 1 - 49\n",
      "Test weeks: 50 - 53\n",
      "Train: 5496, Test: 343\n",
      "\n",
      "Training Combined XGBoost model...\n",
      "Performance:\n",
      "  Train - R²: 0.892, MAE: 1.440, RMSE: 1.964\n",
      "  Test  - R²: 0.753, MAE: 2.189, RMSE: 2.924\n",
      "\n",
      "Top 5 Raw Features:\n",
      "   Gls: 0.323\n",
      "   Ast: 0.236\n",
      "  Passes PrgP: 0.064\n",
      "  Expected xG: 0.062\n",
      "   SoT: 0.052\n",
      "\n",
      "==================================================\n",
      "TRAINING POSITION-SPECIFIC MODELS\n",
      "==================================================\n",
      "Clean dataset: 1695 observations\n",
      "Week range: 1 - 53\n",
      "Training weeks: 1 - 49\n",
      "Test weeks: 50 - 53\n",
      "Train: 1585, Test: 110\n",
      "\n",
      "Training Forward XGBoost model...\n",
      "Performance:\n",
      "  Train - R²: 0.999, MAE: 0.199, RMSE: 0.302\n",
      "  Test  - R²: 0.985, MAE: 0.698, RMSE: 0.945\n",
      "\n",
      "Top 5 Raw Features:\n",
      "   Gls: 0.741\n",
      "   Ast: 0.149\n",
      "  Expected xG: 0.060\n",
      "   SoT: 0.031\n",
      "  Min: 0.007\n",
      "Clean dataset: 1823 observations\n",
      "Week range: 1 - 53\n",
      "Training weeks: 1 - 49\n",
      "Test weeks: 50 - 53\n",
      "Train: 1724, Test: 99\n",
      "\n",
      "Training Midfield XGBoost model...\n",
      "Performance:\n",
      "  Train - R²: 0.988, MAE: 0.315, RMSE: 0.442\n",
      "  Test  - R²: 0.827, MAE: 1.134, RMSE: 1.524\n",
      "\n",
      "Top 5 Raw Features:\n",
      "   Ast: 0.694\n",
      "  Expected xAG: 0.146\n",
      "  Passes PrgP: 0.047\n",
      "   Tkl: 0.035\n",
      "  Passes Cmp%: 0.027\n",
      "Clean dataset: 1823 observations\n",
      "Week range: 1 - 53\n",
      "Training weeks: 1 - 49\n",
      "Test weeks: 50 - 53\n",
      "Train: 1718, Test: 105\n",
      "\n",
      "Training Defense XGBoost model...\n",
      "Performance:\n",
      "  Train - R²: 0.973, MAE: 0.524, RMSE: 0.716\n",
      "  Test  - R²: 0.779, MAE: 1.448, RMSE: 1.958\n",
      "\n",
      "Top 5 Raw Features:\n",
      "   Blocks: 0.454\n",
      "   Int: 0.286\n",
      "   Tkl: 0.108\n",
      "  Min: 0.028\n",
      "  Passes PrgP: 0.017\n",
      "Clean dataset: 498 observations\n",
      "Week range: 1 - 53\n",
      "Training weeks: 1 - 49\n",
      "Test weeks: 50 - 53\n",
      "Train: 469, Test: 29\n",
      "\n",
      "Training Goalkeeper XGBoost model...\n",
      "Performance:\n",
      "  Train - R²: 0.988, MAE: 0.064, RMSE: 0.240\n",
      "  Test  - R²: 0.803, MAE: 0.368, RMSE: 0.850\n",
      "\n",
      "Top 5 Raw Features:\n",
      "  Passes Cmp%: 0.791\n",
      "   Tkl: 0.040\n",
      "  Min: 0.040\n",
      "  Take-Ons Succ: 0.040\n",
      "  Expected xAG: 0.037\n",
      "\n",
      "======================================================================\n",
      "RAW STATISTICS MODEL SUMMARY\n",
      "======================================================================\n",
      "Model        Test R²    Test MAE   Test RMSE   Top Feature         \n",
      "----------------------------------------------------------------------\n",
      "Combined     0.753      2.189      2.924        Gls                \n",
      "Forward      0.985      0.698      0.945        Gls                \n",
      "Midfield     0.827      1.134      1.524        Ast                \n",
      "Defense      0.779      1.448      1.958        Blocks             \n",
      "Goalkeeper   0.803      0.368      0.850       Passes Cmp%         \n",
      "\n",
      "✅ Raw statistics models ready!\n",
      "📊 Trained 5 models using only original match data\n",
      "🎯 Use: predict_with_raw_features('Player Name', 'Model Type')\n",
      "\n",
      "📋 Raw features being used (13):\n",
      "   1. Min\n",
      "   2.  Gls\n",
      "   3.  Ast\n",
      "   4.  SoT\n",
      "   5.  Tkl\n",
      "   6.  Int\n",
      "   7.  Blocks\n",
      "   8. Passes Cmp%\n",
      "   9. Expected xG\n",
      "  10. Expected xAG\n",
      "  11. Take-Ons Succ\n",
      "  12. Carries PrgC\n",
      "  13. Passes PrgP\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# XGBOOST WITH RAW MATCH STATISTICS ONLY\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"XGBOOST TRAINING WITH RAW MATCH STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Available columns: {list(df.columns)}\")\n",
    "\n",
    "# ==========================================\n",
    "# SELECT RAW MATCH STATISTICS ONLY\n",
    "# ==========================================\n",
    "\n",
    "# Define the raw match statistics you want to use\n",
    "raw_match_stats = [\n",
    "    'Min',           # Minutes played\n",
    "    ' Gls',          # Goals (note the space)\n",
    "    ' Ast',          # Assists\n",
    "    ' SoT',          # Shots on target\n",
    "    ' KP',           # Key passes\n",
    "    ' Tkl',          # Tackles\n",
    "    ' Int',          # Interceptions\n",
    "    ' Blocks',       # Blocks\n",
    "    ' Clr',          # Clearances\n",
    "    'Passes Cmp%',   # Pass completion %\n",
    "    'Expected xG',   # Expected goals\n",
    "    'Expected xAG',  # Expected assists\n",
    "    'Take-Ons Succ', # Successful take-ons\n",
    "    'Carries PrgC',  # Progressive carries\n",
    "    'Passes PrgP',   # Progressive passes\n",
    "    'Touches'        # Total touches\n",
    "]\n",
    "\n",
    "# Check which features actually exist in your dataset\n",
    "available_features = []\n",
    "missing_features = []\n",
    "\n",
    "for feature in raw_match_stats:\n",
    "    if feature in df.columns:\n",
    "        available_features.append(feature)\n",
    "    else:\n",
    "        missing_features.append(feature)\n",
    "\n",
    "print(f\"\\nAvailable features ({len(available_features)}): {available_features}\")\n",
    "if missing_features:\n",
    "    print(f\"Missing features ({len(missing_features)}): {missing_features}\")\n",
    "\n",
    "# ==========================================\n",
    "# PREPARE DATA FOR TRAINING\n",
    "# ==========================================\n",
    "\n",
    "def prepare_raw_data(df, features, test_weeks=4):\n",
    "    \"\"\"Prepare data using only raw match statistics\"\"\"\n",
    "    \n",
    "    # Remove rows with missing target variable\n",
    "    df_clean = df[df['Rebalanced_Score'].notna()].copy()\n",
    "    \n",
    "    # Create week column if not exists\n",
    "    if 'Week' not in df_clean.columns:\n",
    "        df_clean['Date'] = pd.to_datetime(df_clean['Date'])\n",
    "        df_clean['Week'] = df_clean['Date'].dt.isocalendar().week\n",
    "    \n",
    "    print(f\"Clean dataset: {len(df_clean)} observations\")\n",
    "    print(f\"Week range: {df_clean['Week'].min()} - {df_clean['Week'].max()}\")\n",
    "    \n",
    "    # Time-based split\n",
    "    latest_week = df_clean['Week'].max()\n",
    "    test_start_week = latest_week - test_weeks + 1\n",
    "    \n",
    "    train_data = df_clean[df_clean['Week'] < test_start_week]\n",
    "    test_data = df_clean[df_clean['Week'] >= test_start_week]\n",
    "    \n",
    "    print(f\"Training weeks: {train_data['Week'].min()} - {train_data['Week'].max()}\")\n",
    "    print(f\"Test weeks: {test_data['Week'].min()} - {test_data['Week'].max()}\")\n",
    "    print(f\"Train: {len(train_data)}, Test: {len(test_data)}\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X_train = train_data[features].fillna(0)\n",
    "    y_train = train_data['Rebalanced_Score']\n",
    "    X_test = test_data[features].fillna(0)\n",
    "    y_test = test_data['Rebalanced_Score']\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'features': features\n",
    "    }\n",
    "\n",
    "# ==========================================\n",
    "# TRAIN MODELS WITH RAW STATISTICS\n",
    "# ==========================================\n",
    "\n",
    "# 1. Combined model (all positions)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMBINED MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "combined_raw_data = prepare_raw_data(df, available_features)\n",
    "\n",
    "def train_raw_xgb(data_dict, name):\n",
    "    \"\"\"Train XGBoost with raw features\"\"\"\n",
    "    print(f\"\\nTraining {name} XGBoost model...\")\n",
    "    \n",
    "    X_train = data_dict['X_train']\n",
    "    y_train = data_dict['y_train']\n",
    "    X_test = data_dict['X_test']\n",
    "    y_test = data_dict['y_test']\n",
    "    \n",
    "    # XGBoost model\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    print(f\"Performance:\")\n",
    "    print(f\"  Train - R²: {train_r2:.3f}, MAE: {train_mae:.3f}, RMSE: {train_rmse:.3f}\")\n",
    "    print(f\"  Test  - R²: {test_r2:.3f}, MAE: {test_mae:.3f}, RMSE: {test_rmse:.3f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 5 Raw Features:\")\n",
    "    for _, row in importance.head().iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'importance': importance,\n",
    "        'metrics': {\n",
    "            'train_r2': train_r2, 'test_r2': test_r2,\n",
    "            'train_mae': train_mae, 'test_mae': test_mae,\n",
    "            'train_rmse': train_rmse, 'test_rmse': test_rmse\n",
    "        },\n",
    "        'predictions': {'train': y_train_pred, 'test': y_test_pred},\n",
    "        'actuals': {'train': y_train, 'test': y_test},\n",
    "        'data': data_dict\n",
    "    }\n",
    "\n",
    "# Train combined model\n",
    "raw_models = {}\n",
    "raw_models['Combined'] = train_raw_xgb(combined_raw_data, 'Combined')\n",
    "\n",
    "# 2. Position-specific models with raw features\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING POSITION-SPECIFIC MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for position in ['Forward', 'Midfield', 'Defense', 'Goalkeeper']:\n",
    "    position_data = df[df['Position_Group'] == position]\n",
    "    \n",
    "    if len(position_data) < 50:  # Need sufficient data\n",
    "        print(f\"Skipping {position} - insufficient data ({len(position_data)} samples)\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        pos_data = prepare_raw_data(position_data, available_features)\n",
    "        raw_models[position] = train_raw_xgb(pos_data, position)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {position} model: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# COMPARISON: RAW vs ENGINEERED FEATURES\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RAW STATISTICS MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"{'Model':<12} {'Test R²':<10} {'Test MAE':<10} {'Test RMSE':<11} {'Top Feature':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, model_info in raw_models.items():\n",
    "    metrics = model_info['metrics']\n",
    "    top_feature = model_info['importance'].iloc[0]['feature']\n",
    "    \n",
    "    print(f\"{name:<12} {metrics['test_r2']:<10.3f} {metrics['test_mae']:<10.3f} \"\n",
    "          f\"{metrics['test_rmse']:<11.3f} {top_feature:<20}\")\n",
    "\n",
    "# ==========================================\n",
    "# SIMPLE PREDICTION FUNCTION\n",
    "# ==========================================\n",
    "\n",
    "def predict_with_raw_features(player_name, model_type='Combined'):\n",
    "    \"\"\"Predict using raw match statistics only\"\"\"\n",
    "    \n",
    "    if model_type not in raw_models:\n",
    "        print(f\"Model {model_type} not available\")\n",
    "        available = list(raw_models.keys())\n",
    "        print(f\"Available models: {available}\")\n",
    "        return None\n",
    "    \n",
    "    model_info = raw_models[model_type]\n",
    "    test_data = model_info['data']['test_data']\n",
    "    \n",
    "    # Find player\n",
    "    player_matches = test_data[test_data['Player'].str.contains(player_name, case=False, na=False)]\n",
    "    \n",
    "    if len(player_matches) == 0:\n",
    "        print(f\"Player '{player_name}' not found in {model_type} test data\")\n",
    "        return None\n",
    "    \n",
    "    # Use most recent match\n",
    "    latest_match = player_matches.iloc[-1]\n",
    "    \n",
    "    # Get raw features\n",
    "    features = model_info['data']['features']\n",
    "    X_latest = latest_match[features].fillna(0).values.reshape(1, -1)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model_info['model'].predict(X_latest)[0]\n",
    "    actual = latest_match['Rebalanced_Score']\n",
    "    \n",
    "    print(f\"\\n🎯 {player_name} Prediction (Raw Features - {model_type}):\")\n",
    "    print(f\"   Date: {latest_match['Date']}\")\n",
    "    print(f\"   Actual score: {actual:.3f}\")\n",
    "    print(f\"   Predicted: {prediction:.3f}\")\n",
    "    print(f\"   Difference: {abs(actual - prediction):.3f}\")\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "print(f\"\\n✅ Raw statistics models ready!\")\n",
    "print(f\"📊 Trained {len(raw_models)} models using only original match data\")\n",
    "print(f\"🎯 Use: predict_with_raw_features('Player Name', 'Model Type')\")\n",
    "\n",
    "# Show what raw features are being used\n",
    "print(f\"\\n📋 Raw features being used ({len(available_features)}):\")\n",
    "for i, feature in enumerate(available_features):\n",
    "    print(f\"  {i+1:2d}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19da2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/5yylt10156q8bq2drpk08r7w0000gn/T/ipykernel_54019/1983041645.py:101: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  schedule_df['Date'] = pd.to_datetime(schedule_df[date_cols[0]], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING TEAM-LEVEL WIN/LOSS DATASET FOR LOGISTIC REGRESSION\n",
      "============================================================\n",
      "Loading schedule files...\n",
      "✓ Loading: real_madrid_schedule_17_18.csv\n",
      "✓ Loading: real_madrid_schedule_18_19.csv\n",
      "✓ Loading: real_madrid_schedule_19_20.csv\n",
      "✓ Loading: real_madrid_schedule_20_21.csv\n",
      "✓ Loading: real_madrid_schedule_21_22.csv\n",
      "✓ Loading: real_madrid_schedule_22_23.csv\n",
      "✓ Loading: real_madrid_schedule_23_24 (1).csv\n",
      "✓ Loading: real_madrid_schedule_24_25 (1).csv\n",
      "Combined schedule shape: (418, 21)\n",
      "Columns: ['Date', 'Time', 'Comp', 'Round', 'Day', 'Venue', 'Result', 'GF', 'GA', 'Opponent', 'xG', 'xGA', 'Poss', 'Attendance', 'Captain', 'Formation', 'Opp Formation', 'Referee', 'Match Report', 'Notes', 'Season']\n",
      "\n",
      "Loading rebalanced scores from: /Users/home/Capstone/ADS599_Capstone/Main Notebook/Code Library Folder/02_Feature_Engineering/outputs/real_madrid_rebalanced_scores.csv\n",
      "✓ Rebalanced scores shape: (7140, 80)\n",
      "Columns: ['Date', 'Competition', 'Opponent', 'Player', '#', 'Nation', 'Pos', 'Age', 'Min', ' Gls', ' Ast', ' PK', ' PKatt', ' Sh', ' SoT', ' CrdY', ' CrdR', ' Int', 'Match URL', 'Season', ' Touches', ' Tkl', ' Blocks', 'Expected xG', 'Expected npxG', 'Expected xAG', 'SCA', 'GCA', 'Passes Cmp', 'Passes Att', 'Passes Cmp%', 'Passes PrgP', 'Carries Carries', 'Carries PrgC', 'Take-Ons Att', 'Take-Ons Succ', 'Tackles Tkl', 'Tackles TklW', 'Tackles Def 3rd', 'Tackles Mid 3rd', 'Tackles Att 3rd', 'Challenges Tkl', 'Challenges Att', 'Challenges Tkl%', 'Challenges Lost', 'Blocks Blocks', 'Blocks Sh', 'Blocks Pass', 'Int', 'Tkl+Int', 'Clr', 'Err', 'Total Cmp', 'Total Att', 'Total Cmp%', 'Total TotDist', 'Total PrgDist', 'Short Cmp', 'Short Att', 'Short Cmp%', 'Medium Cmp', 'Medium Att', 'Medium Cmp%', 'Long Cmp', 'Long Att', 'Long Cmp%', 'Ast', 'xAG', 'xA', 'KP', '3-Jan', 'PPA', 'CrsPA', 'PrgP', 'SCA SCA', 'SCA GCA', ' 1/3', 'Position_Group', 'Rebalanced_Score', 'Week']\n",
      "\n",
      "Schedule data preview:\n",
      "      Date   Time          Comp        Round  Day Venue Result GF GA  \\\n",
      "0  8/20/17  22:15       La Liga  Matchweek 1  Sun  Away      W  3  0   \n",
      "1  8/27/17  22:15       La Liga  Matchweek 2  Sun  Home      D  2  2   \n",
      "2   9/9/17  13:00       La Liga  Matchweek 3  Sat  Home      D  1  1   \n",
      "3  9/13/17  20:45  Champions Lg  Group stage  Wed  Home      W  3  0   \n",
      "4  9/17/17  20:45       La Liga  Matchweek 4  Sun  Away      W  3  1   \n",
      "\n",
      "        Opponent  ...  xGA  Poss  Attendance       Captain  Formation  \\\n",
      "0      La Coruña  ...  1.7  68.0     27770.0  Sergio Ramos    4-3-1-2   \n",
      "1       Valencia  ...  0.9  66.0     65107.0       Marcelo      4-3-3   \n",
      "2        Levante  ...  0.7  68.0     67789.0  Sergio Ramos    4-2-3-1   \n",
      "3    cy APOEL FC  ...  0.2  69.0     71060.0  Sergio Ramos  4-1-2-1-2   \n",
      "4  Real Sociedad  ...  0.5  50.0     24966.0  Sergio Ramos      4-3-3   \n",
      "\n",
      "  Opp Formation              Referee  Match Report Notes Season  \n",
      "0       4-2-3-1        José González  Match Report   NaN     18  \n",
      "1         4-4-2      David Fernández  Match Report   NaN     18  \n",
      "2         4-5-1  Alejandro Hernández  Match Report   NaN     18  \n",
      "3         4-4-2       Benoît Bastien  Match Report   NaN     18  \n",
      "4         4-3-3     Ignacio Iglesias  Match Report   NaN     18  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Rebalanced scores preview:\n",
      "         Date Competition        Opponent             Player     #  Nation  \\\n",
      "0  2015-08-23     La Liga  Sporting Gijon               Jesé  20.0  es ESP   \n",
      "1  2015-08-23     La Liga  Sporting Gijon    James Rodríguez  10.0  co COL   \n",
      "2  2015-08-23     La Liga  Sporting Gijon  Cristiano Ronaldo   7.0  pt POR   \n",
      "3  2015-08-23     La Liga  Sporting Gijon               Isco  22.0  es ESP   \n",
      "4  2015-08-23     La Liga  Sporting Gijon      Mateo Kovačić  16.0  hr CRO   \n",
      "\n",
      "     Pos     Age   Min   Gls  ...  3-Jan  PPA  CrsPA  PrgP  SCA SCA  SCA GCA  \\\n",
      "0     FW  22-178  55.0   0.0  ...    NaN  NaN    NaN   NaN      NaN      NaN   \n",
      "1  FW,MF  24-042  35.0   0.0  ...    NaN  NaN    NaN   NaN      NaN      NaN   \n",
      "2     AM  30-199  90.0   0.0  ...    NaN  NaN    NaN   NaN      NaN      NaN   \n",
      "3     AM  23-124  70.0   0.0  ...    NaN  NaN    NaN   NaN      NaN      NaN   \n",
      "4     MF  21-109  20.0   0.0  ...    NaN  NaN    NaN   NaN      NaN      NaN   \n",
      "\n",
      "    1/3  Position_Group Rebalanced_Score Week  \n",
      "0   NaN         Forward              NaN   34  \n",
      "1   NaN         Forward              NaN   34  \n",
      "2   NaN        Midfield              NaN   34  \n",
      "3   NaN        Midfield              NaN   34  \n",
      "4   NaN        Midfield              NaN   34  \n",
      "\n",
      "[5 rows x 80 columns]\n",
      "\n",
      "Processing dates...\n",
      "Using result column: Result\n",
      "Using score column: Rebalanced_Score\n",
      "\n",
      "📊 AGGREGATING TEAM PERFORMANCE BY MATCH...\n",
      "Team aggregated data shape: (499, 4)\n",
      "Team scores preview:\n",
      "        Date  Team_Rebalanced_Score_Sum  Team_Rebalanced_Score_Mean  \\\n",
      "0 2015-08-23                       16.0                        16.0   \n",
      "1 2015-08-29                       16.0                        16.0   \n",
      "2 2015-09-12                       16.0                        16.0   \n",
      "3 2015-09-15                       16.0                        16.0   \n",
      "4 2015-09-19                       16.0                        16.0   \n",
      "\n",
      "   Players_Count  \n",
      "0              1  \n",
      "1              1  \n",
      "2              1  \n",
      "3              1  \n",
      "4              1  \n",
      "\n",
      "Team score statistics:\n",
      "Sum - Range: 16.00 to 210.45\n",
      "Mean - Range: 4.82 to 16.00\n",
      "Average players per match: 11.7\n",
      "\n",
      "Merging schedule with TEAM-LEVEL scores...\n",
      "Merged dataset shape: (397, 5)\n",
      "\n",
      "Filtering Win/Loss matches...\n",
      "Final dataset shape: (323, 6)\n",
      "Result distribution:\n",
      "Result\n",
      "W    256\n",
      "L     67\n",
      "Name: count, dtype: int64\n",
      "Win distribution:\n",
      "Win\n",
      "1    256\n",
      "0     67\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Choose which TEAM score to use for logistic regression:\n",
      "1. Team_Rebalanced_Score_Sum (total team performance)\n",
      "2. Team_Rebalanced_Score_Mean (average player performance)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Paths\n",
    "season_dir = \"/Users/home/Capstone/ADS599_Capstone/Main Notebook/Data Folder/DataExtracted/\"\n",
    "rebalanced_scores_path = \"/Users/home/Capstone/ADS599_Capstone/Main Notebook/Code Library Folder/02_Feature_Engineering/outputs/real_madrid_rebalanced_scores.csv\"\n",
    "\n",
    "season_files = [\n",
    "    \"real_madrid_schedule_17_18.csv\",\n",
    "    \"real_madrid_schedule_18_19.csv\",\n",
    "    \"real_madrid_schedule_19_20.csv\",\n",
    "    \"real_madrid_schedule_20_21.csv\",\n",
    "    \"real_madrid_schedule_21_22.csv\",\n",
    "    \"real_madrid_schedule_22_23.csv\",\n",
    "    \"real_madrid_schedule_23_24 (1).csv\",\n",
    "    \"real_madrid_schedule_24_25 (1).csv\"\n",
    "]\n",
    "\n",
    "def load_and_combine_schedules():\n",
    "    \"\"\"\n",
    "    Load and combine all schedule files\n",
    "    \"\"\"\n",
    "    print(\"Loading schedule files...\")\n",
    "    \n",
    "    all_schedules = []\n",
    "    \n",
    "    for file in season_files:\n",
    "        file_path = os.path.join(season_dir, file)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"✓ Loading: {file}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Add season info\n",
    "            season = file.split('_')[-1].replace('.csv', '').replace(' (1)', '')\n",
    "            df['Season'] = season\n",
    "            \n",
    "            all_schedules.append(df)\n",
    "        else:\n",
    "            print(f\"❌ File not found: {file}\")\n",
    "    \n",
    "    # Combine all schedules\n",
    "    combined_schedule = pd.concat(all_schedules, ignore_index=True)\n",
    "    print(f\"Combined schedule shape: {combined_schedule.shape}\")\n",
    "    print(f\"Columns: {list(combined_schedule.columns)}\")\n",
    "    \n",
    "    return combined_schedule\n",
    "\n",
    "def load_rebalanced_scores():\n",
    "    \"\"\"\n",
    "    Load rebalanced scores data\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading rebalanced scores from: {rebalanced_scores_path}\")\n",
    "    \n",
    "    if os.path.exists(rebalanced_scores_path):\n",
    "        scores_df = pd.read_csv(rebalanced_scores_path)\n",
    "        print(f\"✓ Rebalanced scores shape: {scores_df.shape}\")\n",
    "        print(f\"Columns: {list(scores_df.columns)}\")\n",
    "        return scores_df\n",
    "    else:\n",
    "        print(f\"❌ Rebalanced scores file not found!\")\n",
    "        return None\n",
    "\n",
    "def create_win_loss_dataset():\n",
    "    \"\"\"\n",
    "    Create dataset with Date, Win/Loss, and TEAM-LEVEL Rebalanced_Score\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"CREATING TEAM-LEVEL WIN/LOSS DATASET FOR LOGISTIC REGRESSION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load schedule data\n",
    "    schedule_df = load_and_combine_schedules()\n",
    "    \n",
    "    # Load rebalanced scores\n",
    "    scores_df = load_rebalanced_scores()\n",
    "    \n",
    "    if scores_df is None:\n",
    "        print(\"❌ Cannot proceed without rebalanced scores\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nSchedule data preview:\")\n",
    "    print(schedule_df.head())\n",
    "    \n",
    "    print(f\"\\nRebalanced scores preview:\")\n",
    "    print(scores_df.head())\n",
    "    \n",
    "    # Clean and standardize date formats\n",
    "    print(\"\\nProcessing dates...\")\n",
    "    \n",
    "    # For schedule data - try to find date column\n",
    "    date_cols = [col for col in schedule_df.columns if 'date' in col.lower()]\n",
    "    if date_cols:\n",
    "        schedule_df['Date'] = pd.to_datetime(schedule_df[date_cols[0]], errors='coerce')\n",
    "    else:\n",
    "        print(\"Available schedule columns:\", list(schedule_df.columns))\n",
    "        date_col = input(\"Enter the name of the DATE column in schedule data: \")\n",
    "        schedule_df['Date'] = pd.to_datetime(schedule_df[date_col], errors='coerce')\n",
    "    \n",
    "    # For scores data - try to find date column\n",
    "    date_cols_scores = [col for col in scores_df.columns if 'date' in col.lower()]\n",
    "    if date_cols_scores:\n",
    "        scores_df['Date'] = pd.to_datetime(scores_df[date_cols_scores[0]], errors='coerce')\n",
    "    else:\n",
    "        print(\"Available rebalanced scores columns:\", list(scores_df.columns))\n",
    "        date_col = input(\"Enter the name of the DATE column in rebalanced scores: \")\n",
    "        scores_df['Date'] = pd.to_datetime(scores_df[date_col], errors='coerce')\n",
    "    \n",
    "    # Find result column in schedule\n",
    "    result_cols = [col for col in schedule_df.columns if any(word in col.lower() for word in ['result', 'outcome'])]\n",
    "    if result_cols:\n",
    "        result_col = result_cols[0]\n",
    "    else:\n",
    "        print(\"Available schedule columns:\", list(schedule_df.columns))\n",
    "        result_col = input(\"Enter the name of the RESULT column (W/L/D): \")\n",
    "    \n",
    "    # Find rebalanced score column\n",
    "    score_cols = [col for col in scores_df.columns if 'rebalanced' in col.lower() or 'score' in col.lower()]\n",
    "    if score_cols:\n",
    "        score_col = score_cols[0]\n",
    "    else:\n",
    "        print(\"Available rebalanced scores columns:\", list(scores_df.columns))\n",
    "        score_col = input(\"Enter the name of the REBALANCED SCORE column: \")\n",
    "    \n",
    "    print(f\"Using result column: {result_col}\")\n",
    "    print(f\"Using score column: {score_col}\")\n",
    "    \n",
    "    # Create clean schedule dataframe\n",
    "    schedule_clean = schedule_df[['Date', result_col]].copy()\n",
    "    schedule_clean.columns = ['Date', 'Result']\n",
    "    schedule_clean = schedule_clean.dropna()\n",
    "    \n",
    "    # AGGREGATE REBALANCED SCORES BY MATCH (TEAM LEVEL)\n",
    "    print(f\"\\n📊 AGGREGATING TEAM PERFORMANCE BY MATCH...\")\n",
    "    \n",
    "    # Group by date and sum/average the rebalanced scores for the entire team\n",
    "    team_scores = scores_df.groupby('Date').agg({\n",
    "        score_col: ['sum', 'mean', 'count']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    team_scores.columns = ['Date', 'Team_Rebalanced_Score_Sum', 'Team_Rebalanced_Score_Mean', 'Players_Count']\n",
    "    \n",
    "    print(f\"Team aggregated data shape: {team_scores.shape}\")\n",
    "    print(\"Team scores preview:\")\n",
    "    print(team_scores.head())\n",
    "    \n",
    "    print(f\"\\nTeam score statistics:\")\n",
    "    print(f\"Sum - Range: {team_scores['Team_Rebalanced_Score_Sum'].min():.2f} to {team_scores['Team_Rebalanced_Score_Sum'].max():.2f}\")\n",
    "    print(f\"Mean - Range: {team_scores['Team_Rebalanced_Score_Mean'].min():.2f} to {team_scores['Team_Rebalanced_Score_Mean'].max():.2f}\")\n",
    "    print(f\"Average players per match: {team_scores['Players_Count'].mean():.1f}\")\n",
    "    \n",
    "    # Merge schedule with team-level scores\n",
    "    print(\"\\nMerging schedule with TEAM-LEVEL scores...\")\n",
    "    merged_df = pd.merge(schedule_clean, team_scores, on='Date', how='inner')\n",
    "    \n",
    "    print(f\"Merged dataset shape: {merged_df.shape}\")\n",
    "    \n",
    "    if merged_df.empty:\n",
    "        print(\"❌ No matching dates found between schedule and team scores!\")\n",
    "        print(\"Schedule date range:\", schedule_clean['Date'].min(), \"to\", schedule_clean['Date'].max())\n",
    "        print(\"Team scores date range:\", team_scores['Date'].min(), \"to\", team_scores['Date'].max())\n",
    "        return None\n",
    "    \n",
    "    # Filter for Win/Loss only (remove draws)\n",
    "    print(\"\\nFiltering Win/Loss matches...\")\n",
    "    win_loss_df = merged_df[merged_df['Result'].isin(['W', 'L'])].copy()\n",
    "    \n",
    "    # Create binary target\n",
    "    win_loss_df['Win'] = (win_loss_df['Result'] == 'W').astype(int)\n",
    "    \n",
    "    print(f\"Final dataset shape: {win_loss_df.shape}\")\n",
    "    print(\"Result distribution:\")\n",
    "    print(win_loss_df['Result'].value_counts())\n",
    "    print(\"Win distribution:\")\n",
    "    print(win_loss_df['Win'].value_counts())\n",
    "    \n",
    "    # Ask user which team score to use for analysis\n",
    "    print(f\"\\nChoose which TEAM score to use for logistic regression:\")\n",
    "    print(f\"1. Team_Rebalanced_Score_Sum (total team performance)\")\n",
    "    print(f\"2. Team_Rebalanced_Score_Mean (average player performance)\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"2\":\n",
    "        score_column = 'Team_Rebalanced_Score_Mean'\n",
    "        print(\"✓ Using MEAN team score (average player performance)\")\n",
    "    else:\n",
    "        score_column = 'Team_Rebalanced_Score_Sum'\n",
    "        print(\"✓ Using SUM team score (total team performance)\")\n",
    "    \n",
    "    # Final clean dataset with chosen score\n",
    "    final_df = win_loss_df[['Date', 'Result', score_column, 'Players_Count', 'Win']].copy()\n",
    "    final_df.rename(columns={score_column: 'Rebalanced_Score'}, inplace=True)\n",
    "    \n",
    "    print(f\"\\nFinal team-level dataset:\")\n",
    "    print(final_df.head(10))\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def create_train_test_splits(df):\n",
    "    \"\"\"\n",
    "    Create training and test splits\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"CREATING TRAIN/TEST SPLITS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Features and target\n",
    "    X = df[['Rebalanced_Score']].copy()\n",
    "    y = df['Win'].copy()\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    \n",
    "    # Split data: 70% train, 30% test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} matches\")\n",
    "    print(f\"Test set: {X_test.shape[0]} matches\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def plot_logistic_curve(df, model, scaler):\n",
    "    \"\"\"\n",
    "    Plot the S-curve showing how Rebalanced_Score predicts Win probability\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PLOTTING LOGISTIC S-CURVE\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # First, let's analyze the data distribution\n",
    "    print(\"DATA ANALYSIS:\")\n",
    "    print(f\"Rebalanced Score range: {df['Rebalanced_Score'].min():.2f} to {df['Rebalanced_Score'].max():.2f}\")\n",
    "    print(f\"Mean: {df['Rebalanced_Score'].mean():.2f}, Std: {df['Rebalanced_Score'].std():.2f}\")\n",
    "    print(f\"Win rate: {df['Win'].mean():.2%}\")\n",
    "    \n",
    "    # Check if we have enough variation\n",
    "    if df['Rebalanced_Score'].std() < 1:\n",
    "        print(\"⚠️  WARNING: Low variation in Rebalanced_Score - S-curve may be flat\")\n",
    "    \n",
    "    # Create WIDER range of Rebalanced_Score values for plotting\n",
    "    score_min = df['Rebalanced_Score'].min() - df['Rebalanced_Score'].std()\n",
    "    score_max = df['Rebalanced_Score'].max() + df['Rebalanced_Score'].std()\n",
    "    score_range = np.linspace(score_min, score_max, 200).reshape(-1, 1)\n",
    "    \n",
    "    # Scale the range using the same scaler\n",
    "    score_range_scaled = scaler.transform(score_range)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    win_probabilities = model.predict_proba(score_range_scaled)[:, 1]\n",
    "    \n",
    "    print(f\"Predicted probability range: {win_probabilities.min():.3f} to {win_probabilities.max():.3f}\")\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot the S-curve with better scaling\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(score_range, win_probabilities, 'b-', linewidth=3, label='Logistic Curve')\n",
    "    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='50% Probability')\n",
    "    \n",
    "    # Add actual data points with jitter for better visibility\n",
    "    wins = df[df['Win'] == 1]['Rebalanced_Score']\n",
    "    losses = df[df['Win'] == 0]['Rebalanced_Score']\n",
    "    \n",
    "    # Add some jitter to y-values to see overlapping points\n",
    "    win_jitter = np.random.normal(1, 0.02, len(wins))\n",
    "    loss_jitter = np.random.normal(0, 0.02, len(losses))\n",
    "    \n",
    "    plt.scatter(wins, win_jitter, color='green', alpha=0.6, s=30, label=f'Wins ({len(wins)})')\n",
    "    plt.scatter(losses, loss_jitter, color='red', alpha=0.6, s=30, label=f'Losses ({len(losses)})')\n",
    "    \n",
    "    plt.xlabel('Rebalanced Score')\n",
    "    plt.ylabel('Win Probability')\n",
    "    plt.title('S-Curve: Rebalanced Score vs Win Probability')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    \n",
    "    # Distribution of Rebalanced Scores by result\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.hist(losses, bins=15, alpha=0.7, color='red', label=f'Losses ({len(losses)})', density=True)\n",
    "    plt.hist(wins, bins=15, alpha=0.7, color='green', label=f'Wins ({len(wins)})', density=True)\n",
    "    plt.xlabel('Rebalanced Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Distribution of Rebalanced Scores')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot comparison\n",
    "    plt.subplot(2, 3, 3)\n",
    "    box_data = [losses, wins]\n",
    "    box_plot = plt.boxplot(box_data, labels=['Losses', 'Wins'], patch_artist=True)\n",
    "    box_plot['boxes'][0].set_facecolor('red')\n",
    "    box_plot['boxes'][1].set_facecolor('green')\n",
    "    plt.ylabel('Rebalanced Score')\n",
    "    plt.title('Rebalanced Score by Result')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Show the logistic function equation\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.text(0.1, 0.8, f\"Logistic Regression Equation:\", fontsize=12, fontweight='bold')\n",
    "    plt.text(0.1, 0.6, f\"P(Win) = 1 / (1 + e^-(β₀ + β₁×Score))\", fontsize=11)\n",
    "    plt.text(0.1, 0.4, f\"Where:\", fontsize=11)\n",
    "    plt.text(0.1, 0.3, f\"β₀ (intercept) = {model.intercept_[0]:.4f}\", fontsize=10)\n",
    "    plt.text(0.1, 0.2, f\"β₁ (coefficient) = {model.coef_[0][0]:.4f}\", fontsize=10)\n",
    "    plt.text(0.1, 0.05, f\"Interpretation: Each 1-unit increase in\\nRebalanced Score multiplies odds by {np.exp(model.coef_[0][0]):.3f}\", fontsize=9)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axis('off')\n",
    "    plt.title('Model Parameters')\n",
    "    \n",
    "    # Probability bins analysis\n",
    "    plt.subplot(2, 3, 5)\n",
    "    \n",
    "    # Create probability predictions for actual data\n",
    "    X_actual = df[['Rebalanced_Score']].values\n",
    "    X_actual_scaled = scaler.transform(X_actual)\n",
    "    y_pred_proba_actual = model.predict_proba(X_actual_scaled)[:, 1]\n",
    "    \n",
    "    # Create bins and calculate actual win rate\n",
    "    bins = np.linspace(0, 1, 11)  # 10 bins from 0 to 1\n",
    "    \n",
    "    actual_win_rates = []\n",
    "    predicted_probs = []\n",
    "    \n",
    "    for i in range(len(bins)-1):\n",
    "        mask = (y_pred_proba_actual >= bins[i]) & (y_pred_proba_actual < bins[i+1])\n",
    "        if mask.sum() > 0:\n",
    "            actual_rate = df.loc[mask, 'Win'].mean()\n",
    "            predicted_prob = y_pred_proba_actual[mask].mean()\n",
    "            actual_win_rates.append(actual_rate)\n",
    "            predicted_probs.append(predicted_prob)\n",
    "    \n",
    "    # Plot calibration\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.7, label='Perfect Calibration')\n",
    "    if predicted_probs:\n",
    "        plt.plot(predicted_probs, actual_win_rates, 'bo-', markersize=8, label='Model Calibration')\n",
    "    plt.xlabel('Predicted Win Probability')\n",
    "    plt.ylabel('Actual Win Rate')\n",
    "    plt.title('Model Calibration')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Score vs Probability scatter\n",
    "    plt.subplot(2, 3, 6)\n",
    "    colors = ['red' if w == 0 else 'green' for w in df['Win']]\n",
    "    plt.scatter(df['Rebalanced_Score'], y_pred_proba_actual, c=colors, alpha=0.6)\n",
    "    plt.xlabel('Rebalanced Score')\n",
    "    plt.ylabel('Predicted Win Probability')\n",
    "    plt.title('Score vs Predicted Probability')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(df['Rebalanced_Score'], y_pred_proba_actual, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(df['Rebalanced_Score'], p(df['Rebalanced_Score']), \"b--\", alpha=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed insights\n",
    "    print(f\"\\nDETAILED ANALYSIS:\")\n",
    "    print(f\"─\" * 50)\n",
    "    \n",
    "    # Statistical significance\n",
    "    wins_scores = df[df['Win'] == 1]['Rebalanced_Score']\n",
    "    losses_scores = df[df['Win'] == 0]['Rebalanced_Score']\n",
    "    \n",
    "    from scipy import stats\n",
    "    t_stat, p_value = stats.ttest_ind(wins_scores, losses_scores)\n",
    "    \n",
    "    print(f\"📊 Win scores mean: {wins_scores.mean():.2f} ± {wins_scores.std():.2f}\")\n",
    "    print(f\"📊 Loss scores mean: {losses_scores.mean():.2f} ± {losses_scores.std():.2f}\")\n",
    "    print(f\"📊 Difference: {wins_scores.mean() - losses_scores.mean():.2f}\")\n",
    "    print(f\"📊 T-test p-value: {p_value:.4f} ({'Significant' if p_value < 0.05 else 'Not significant'})\")\n",
    "    \n",
    "    # Find score thresholds\n",
    "    threshold_50 = None\n",
    "    for score, prob in zip(score_range.flatten(), win_probabilities):\n",
    "        if prob >= 0.5 and threshold_50 is None:\n",
    "            threshold_50 = score\n",
    "            break\n",
    "    \n",
    "    if threshold_50:\n",
    "        print(f\"🎯 Rebalanced Score ≥ {threshold_50:.2f} → Win probability ≥ 50%\")\n",
    "    \n",
    "    # Score ranges analysis\n",
    "    q33 = df['Rebalanced_Score'].quantile(0.33)\n",
    "    q67 = df['Rebalanced_Score'].quantile(0.67)\n",
    "    \n",
    "    low_scores = df[df['Rebalanced_Score'] < q33]\n",
    "    mid_scores = df[(df['Rebalanced_Score'] >= q33) & (df['Rebalanced_Score'] < q67)]\n",
    "    high_scores = df[df['Rebalanced_Score'] >= q67]\n",
    "    \n",
    "    print(f\"🔻 Low scores (< {q33:.1f}): {low_scores['Win'].mean():.1%} win rate ({len(low_scores)} matches)\")\n",
    "    print(f\"➖ Mid scores ({q33:.1f}-{q67:.1f}): {mid_scores['Win'].mean():.1%} win rate ({len(mid_scores)} matches)\") \n",
    "    print(f\"🔺 High scores (> {q67:.1f}): {high_scores['Win'].mean():.1%} win rate ({len(high_scores)} matches)\")\n",
    "    \n",
    "    # Model strength assessment\n",
    "    prob_range = win_probabilities.max() - win_probabilities.min()\n",
    "    if prob_range < 0.3:\n",
    "        print(f\"⚠️  Model shows WEAK discrimination (probability range: {prob_range:.3f})\")\n",
    "    elif prob_range < 0.6:\n",
    "        print(f\"📈 Model shows MODERATE discrimination (probability range: {prob_range:.3f})\")\n",
    "    else:\n",
    "        print(f\"🚀 Model shows STRONG discrimination (probability range: {prob_range:.3f})\")\n",
    "\n",
    "def run_logistic_regression(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Run logistic regression and show results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"LOGISTIC REGRESSION RESULTS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Fit model\n",
    "    lr_model = LogisticRegression(random_state=42)\n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = lr_model.predict(X_test_scaled)\n",
    "    y_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Results\n",
    "    train_acc = lr_model.score(X_train_scaled, y_train)\n",
    "    test_acc = lr_model.score(X_test_scaled, y_test)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"Training Accuracy: {train_acc:.3f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "    print(f\"ROC AUC Score: {auc_score:.3f}\")\n",
    "    \n",
    "    print(f\"\\nRebalanced Score Coefficient: {lr_model.coef_[0][0]:.4f}\")\n",
    "    print(f\"Intercept: {lr_model.intercept_[0]:.4f}\")\n",
    "    \n",
    "    if lr_model.coef_[0][0] > 0:\n",
    "        print(\"✓ Higher Rebalanced Score increases WIN probability\")\n",
    "    else:\n",
    "        print(\"✗ Higher Rebalanced Score decreases WIN probability\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return lr_model, scaler\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    # Create the dataset\n",
    "    final_df = create_win_loss_dataset()\n",
    "    \n",
    "    if final_df is None:\n",
    "        print(\"❌ Failed to create dataset\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n✓ Successfully created dataset with {len(final_df)} matches\")\n",
    "    print(\"\\nDataset preview:\")\n",
    "    print(final_df.head(10))\n",
    "    \n",
    "    # Save the dataset\n",
    "    output_path = \"/Users/mariamoramora/Documents/GitHub/ADS599_Capstone/Main Notebook/Data Folder/win_loss_rebalanced_dataset.csv\"\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n✓ Dataset saved to: {output_path}\")\n",
    "    \n",
    "    # Create train/test splits\n",
    "    X_train, X_test, y_train, y_test = create_train_test_splits(final_df)\n",
    "    \n",
    "    # Run logistic regression\n",
    "    model, scaler = run_logistic_regression(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Plot the S-curve and analysis\n",
    "    plot_logistic_curve(final_df, model, scaler)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUCCESS! Ready for logistic regression analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return final_df, model, scaler\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    final_df, model, scaler = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_schedules():\n",
    "    \"\"\"Load and combine all schedule files\"\"\"\n",
    "    print(\"Loading schedule files...\")\n",
    "    all_schedules = []\n",
    "    \n",
    "    for file in season_files:\n",
    "        file_path = os.path.join(season_dir, file)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"✓ Loading: {file}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            season = file.split('_')[-1].replace('.csv', '').replace(' (1)', '')\n",
    "            df['Season'] = season\n",
    "            all_schedules.append(df)\n",
    "        else:\n",
    "            print(f\"❌ File not found: {file}\")\n",
    "    \n",
    "    combined_schedule = pd.concat(all_schedules, ignore_index=True)\n",
    "    print(f\"Combined schedule shape: {combined_schedule.shape}\")\n",
    "    return combined_schedule\n",
    "\n",
    "def load_rebalanced_scores():\n",
    "    \"\"\"Load rebalanced scores data\"\"\"\n",
    "    print(f\"\\nLoading rebalanced scores from: {rebalanced_scores_path}\")\n",
    "    \n",
    "    if os.path.exists(rebalanced_scores_path):\n",
    "        scores_df = pd.read_csv(rebalanced_scores_path)\n",
    "        print(f\"✓ Rebalanced scores shape: {scores_df.shape}\")\n",
    "        print(f\"Columns: {list(scores_df.columns)}\")\n",
    "        return scores_df\n",
    "    else:\n",
    "        print(f\"❌ Rebalanced scores file not found!\")\n",
    "        return None\n",
    "\n",
    "def classify_position(pos):\n",
    "    \"\"\"Classify position into Forward, Midfielder, Defender\"\"\"\n",
    "    if pd.isna(pos):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    pos_str = str(pos).upper()\n",
    "    \n",
    "    # Forward positions\n",
    "    if any(keyword in pos_str for keyword in ['FW', 'CF', 'LW', 'RW', 'ST', 'FORWARD']):\n",
    "        return 'Forward'\n",
    "    # Midfielder positions  \n",
    "    elif any(keyword in pos_str for keyword in ['MF', 'CM', 'DM', 'AM', 'LM', 'RM', 'CDM', 'CAM', 'MIDFIELDER']):\n",
    "        return 'Midfielder'\n",
    "    # Defender positions\n",
    "    elif any(keyword in pos_str for keyword in ['DF', 'CB', 'LB', 'RB', 'WB', 'SW', 'DEFENDER']):\n",
    "        return 'Defender'\n",
    "    # Goalkeeper\n",
    "    elif any(keyword in pos_str for keyword in ['GK', 'GOALKEEPER']):\n",
    "        return 'Goalkeeper'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "def create_position_based_dataset():\n",
    "    \"\"\"Create dataset with position-based analysis including multiple features\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"CREATING POSITION-BASED WIN/LOSS DATASET WITH MULTIPLE FEATURES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load data\n",
    "    schedule_df = load_and_combine_schedules()\n",
    "    scores_df = load_rebalanced_scores()\n",
    "    \n",
    "    if scores_df is None:\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nAvailable columns in rebalanced scores:\")\n",
    "    print(scores_df.columns.tolist())\n",
    "    \n",
    "    # Process dates\n",
    "    print(\"\\nProcessing dates...\")\n",
    "    date_cols = [col for col in schedule_df.columns if 'date' in col.lower()]\n",
    "    if date_cols:\n",
    "        schedule_df['Date'] = pd.to_datetime(schedule_df[date_cols[0]], errors='coerce')\n",
    "    else:\n",
    "        print(\"Available schedule columns:\", list(schedule_df.columns))\n",
    "        date_col = input(\"Enter the name of the DATE column in schedule data: \")\n",
    "        schedule_df['Date'] = pd.to_datetime(schedule_df[date_col], errors='coerce')\n",
    "    \n",
    "    date_cols_scores = [col for col in scores_df.columns if 'date' in col.lower()]\n",
    "    if date_cols_scores:\n",
    "        scores_df['Date'] = pd.to_datetime(scores_df[date_cols_scores[0]], errors='coerce')\n",
    "    else:\n",
    "        print(\"Available rebalanced scores columns:\", list(scores_df.columns))\n",
    "        date_col = input(\"Enter the name of the DATE column in rebalanced scores: \")\n",
    "        scores_df['Date'] = pd.to_datetime(scores_df[date_col], errors='coerce')\n",
    "    \n",
    "    # Find columns\n",
    "    result_cols = [col for col in schedule_df.columns if any(word in col.lower() for word in ['result', 'outcome'])]\n",
    "    result_col = result_cols[0] if result_cols else input(\"Enter the name of the RESULT column: \")\n",
    "    \n",
    "    pos_cols = [col for col in scores_df.columns if any(word in col.lower() for word in ['pos', 'position'])]\n",
    "    position_col = pos_cols[0] if pos_cols else input(\"Enter the name of the POSITION column: \")\n",
    "    \n",
    "    print(f\"Using: {result_col}, {position_col}\")\n",
    "    \n",
    "    # Clean schedule data\n",
    "    schedule_clean = schedule_df[['Date', result_col]].copy()\n",
    "    schedule_clean.columns = ['Date', 'Result']\n",
    "    schedule_clean = schedule_clean.dropna()\n",
    "    \n",
    "    # Classify positions\n",
    "    print(f\"\\n📊 CLASSIFYING POSITIONS...\")\n",
    "    scores_df['Position_Group'] = scores_df[position_col].apply(classify_position)\n",
    "    \n",
    "    print(\"Position Group Classification:\")\n",
    "    print(scores_df['Position_Group'].value_counts())\n",
    "    \n",
    "    # Define features to aggregate by position based on your rebalanced score formula\n",
    "    forward_features = [\n",
    "        'Gls', 'Goals',  # Goals\n",
    "        'Ast', 'Assists',  # Assists  \n",
    "        'SoT', 'Shots_On_Target',  # Shots on Target\n",
    "        'xG', 'Expected_Goals',  # Expected Goals\n",
    "        'xAG', 'Expected_Assisted_Goals',  # Expected Assisted Goals\n",
    "        'TakeOns', 'Take_Ons_Succ', 'Takeouts_Successful',  # Take-ons\n",
    "        'Rebalanced_Score'  # Original rebalanced score\n",
    "    ]\n",
    "    \n",
    "    midfielder_features = [\n",
    "        'Passes_Cmp%', 'Pass_Completion_Percent', 'Total_Pass_Completion_Percent',  # Pass Completion %\n",
    "        'KP', 'Key_Passes',  # Key Passes\n",
    "        'Tkl', 'Tackles', 'Tackles_Made',  # Tackles\n",
    "        'Carries_PrgC', 'Progressive_Carries',  # Progressive Carries\n",
    "        'Passes_PrgP', 'Progressive_Passes',  # Progressive Passes\n",
    "        'Touches',  # Touches\n",
    "        'Rebalanced_Score'  # Original rebalanced score\n",
    "    ]\n",
    "    \n",
    "    defender_features = [\n",
    "        'Int', 'Interceptions',  # Interceptions\n",
    "        'Blocks', 'Blocks_Made',  # Blocks\n",
    "        'Clr', 'Clearances',  # Clearances\n",
    "        'Tkl_W', 'Tackles_Won',  # Tackles Won\n",
    "        'Tkl_Def', 'Tackles_Defensive_Third',  # Tackles Defensive 3rd\n",
    "        'Tkl_Mid', 'Tackles_Middle_Third',  # Tackles Middle 3rd\n",
    "        'Rebalanced_Score'  # Original rebalanced score\n",
    "    ]\n",
    "    \n",
    "    goalkeeper_features = [\n",
    "        'Total_Cmp%', 'Total_Pass_Completion_Percent',  # Total Pass Completion %\n",
    "        'Err', 'Errors',  # Errors\n",
    "        'Prg_Dist', 'Total_Progressive_Distance',  # Progressive Distance\n",
    "        'Short_Cmp%', 'Short_Pass_Completion_Percent',  # Short Pass Completion %\n",
    "        'Med_Cmp%', 'Medium_Pass_Completion_Percent',  # Medium Pass Completion %\n",
    "        'Total_Cmp', 'Total_Passes_Completed',  # Total Passes Completed\n",
    "        'Rebalanced_Score'  # Original rebalanced score\n",
    "    ]\n",
    "    \n",
    "    # Find which features actually exist in the dataset\n",
    "    available_columns = scores_df.columns.tolist()\n",
    "    print(f\"\\n📊 IDENTIFYING AVAILABLE FEATURES...\")\n",
    "    \n",
    "    def find_available_features(feature_list, feature_type):\n",
    "        found_features = []\n",
    "        for feature in feature_list:\n",
    "            # Try exact match first, then partial matches\n",
    "            if feature in available_columns:\n",
    "                found_features.append(feature)\n",
    "            else:\n",
    "                # Look for similar column names\n",
    "                matches = [col for col in available_columns if any(part in col for part in feature.split('_')) and len(col) > 2]\n",
    "                if matches:\n",
    "                    found_features.extend(matches[:1])  # Take first match\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        unique_features = []\n",
    "        for f in found_features:\n",
    "            if f not in unique_features:\n",
    "                unique_features.append(f)\n",
    "        \n",
    "        print(f\"{feature_type}: {len(unique_features)} features found - {unique_features}\")\n",
    "        return unique_features\n",
    "    \n",
    "    forward_available = find_available_features(forward_features, \"FORWARD\")\n",
    "    midfielder_available = find_available_features(midfielder_features, \"MIDFIELDER\") \n",
    "    defender_available = find_available_features(defender_features, \"DEFENDER\")\n",
    "    goalkeeper_available = find_available_features(goalkeeper_features, \"GOALKEEPER\")\n",
    "    \n",
    "    # Aggregate by position group per match with multiple features\n",
    "    print(f\"\\n📊 AGGREGATING MULTIPLE FEATURES BY POSITION GROUP PER MATCH...\")\n",
    "    \n",
    "    # Create aggregation dictionary for all features\n",
    "    agg_dict = {}\n",
    "    \n",
    "    # Add features for each position if they exist\n",
    "    all_features = list(set(forward_available + midfielder_available + defender_available + goalkeeper_available))\n",
    "    \n",
    "    for feature in all_features:\n",
    "        if feature in available_columns:\n",
    "            agg_dict[feature] = ['sum', 'mean', 'count']\n",
    "    \n",
    "    print(f\"Aggregating {len(agg_dict)} features: {list(agg_dict.keys())}\")\n",
    "    \n",
    "    # Group by date and position, aggregate all features\n",
    "    position_stats = scores_df.groupby(['Date', 'Position_Group']).agg(agg_dict).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    new_columns = ['Date', 'Position_Group']\n",
    "    for feature in agg_dict.keys():\n",
    "        new_columns.extend([f'{feature}_sum', f'{feature}_mean', f'{feature}_count'])\n",
    "    \n",
    "    position_stats.columns = new_columns\n",
    "    \n",
    "    # Create separate dataframes for each position\n",
    "    position_dfs = {}\n",
    "    \n",
    "    for position in ['Forward', 'Midfielder', 'Defender', 'Goalkeeper']:\n",
    "        pos_data = position_stats[position_stats['Position_Group'] == position].copy()\n",
    "        \n",
    "        if len(pos_data) > 0:\n",
    "            # Select relevant features for this position\n",
    "            if position == 'Forward':\n",
    "                relevant_features = [f for f in forward_available if f in agg_dict.keys()]\n",
    "            elif position == 'Midfielder':\n",
    "                relevant_features = [f for f in midfielder_available if f in agg_dict.keys()]\n",
    "            elif position == 'Defender':\n",
    "                relevant_features = [f for f in defender_available if f in agg_dict.keys()]\n",
    "            else:  # Goalkeeper\n",
    "                relevant_features = [f for f in goalkeeper_available if f in agg_dict.keys()]\n",
    "            \n",
    "            # Keep Date and relevant feature columns (sum and mean)\n",
    "            keep_columns = ['Date']\n",
    "            for feature in relevant_features:\n",
    "                keep_columns.extend([f'{feature}_sum', f'{feature}_mean'])\n",
    "            \n",
    "            # Filter columns that actually exist\n",
    "            existing_columns = [col for col in keep_columns if col in pos_data.columns]\n",
    "            pos_subset = pos_data[existing_columns].copy()\n",
    "            \n",
    "            # Rename columns to include position prefix\n",
    "            rename_dict = {}\n",
    "            for col in pos_subset.columns:\n",
    "                if col != 'Date':\n",
    "                    rename_dict[col] = f'{position}_{col}'\n",
    "            \n",
    "            pos_subset.rename(columns=rename_dict, inplace=True)\n",
    "            position_dfs[position] = pos_subset\n",
    "            \n",
    "            print(f\"{position}: {len(pos_subset)} matches, {len(existing_columns)-1} features\")\n",
    "    \n",
    "    # Merge all position dataframes\n",
    "    merged_positions = None\n",
    "    for position, pos_df in position_dfs.items():\n",
    "        if merged_positions is None:\n",
    "            merged_positions = pos_df.copy()\n",
    "        else:\n",
    "            merged_positions = pd.merge(merged_positions, pos_df, on='Date', how='outer')\n",
    "    \n",
    "    # Fill NaN values with 0 (for matches where position didn't play)\n",
    "    merged_positions = merged_positions.fillna(0)\n",
    "    \n",
    "    print(f\"Combined position data shape: {merged_positions.shape}\")\n",
    "    \n",
    "    # Merge with schedule\n",
    "    merged_df = pd.merge(schedule_clean, merged_positions, on='Date', how='inner')\n",
    "    \n",
    "    # Filter Win/Loss only\n",
    "    win_loss_df = merged_df[merged_df['Result'].isin(['W', 'L'])].copy()\n",
    "    win_loss_df['Win'] = (win_loss_df['Result'] == 'W').astype(int)\n",
    "    \n",
    "    print(f\"Final dataset shape: {win_loss_df.shape}\")\n",
    "    print(f\"Total features: {len(win_loss_df.columns) - 3}\")  # Subtract Date, Result, Win\n",
    "    \n",
    "    # Show available features by position\n",
    "    for position in ['Forward', 'Midfielder', 'Defender', 'Goalkeeper']:\n",
    "        pos_columns = [col for col in win_loss_df.columns if col.startswith(f'{position}_')]\n",
    "        if pos_columns:\n",
    "            print(f\"{position} features ({len(pos_columns)}): {pos_columns[:5]}...\" if len(pos_columns) > 5 else f\"{position} features: {pos_columns}\")\n",
    "    \n",
    "    return win_loss_df\n",
    "\n",
    "def plot_forward_analysis_only(df, score_col, model, scaler, position_name):\n",
    "    \"\"\"Plot S-curve, ROC curve, and confusion matrix for Forward analysis\"\"\"\n",
    "    \n",
    "    # Create EXTENDED range to show complete S-curve (0 to 1)\n",
    "    data_std = df[score_col].std()\n",
    "    data_mean = df[score_col].mean()\n",
    "    \n",
    "    # Extend range much further to ensure we reach 0% and 100% probability\n",
    "    score_min = data_mean - 4 * data_std\n",
    "    score_max = data_mean + 4 * data_std\n",
    "    \n",
    "    score_range = np.linspace(score_min, score_max, 500).reshape(-1, 1)\n",
    "    score_range_scaled = scaler.transform(score_range)\n",
    "    win_probabilities = model.predict_proba(score_range_scaled)[:, 1]\n",
    "    \n",
    "    print(f\"{position_name} - Extended range: {score_min:.2f} to {score_max:.2f}\")\n",
    "    print(f\"{position_name} - Probability range: {win_probabilities.min():.3f} to {win_probabilities.max():.3f}\")\n",
    "    \n",
    "    # Create the plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Plot 1: COMPLETE S-CURVE\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(score_range, win_probabilities, 'b-', linewidth=4, label='Complete S-Curve')\n",
    "    ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='50% Probability')\n",
    "    ax1.axhline(y=0.1, color='orange', linestyle=':', alpha=0.5, label='10%')\n",
    "    ax1.axhline(y=0.9, color='orange', linestyle=':', alpha=0.5, label='90%')\n",
    "    \n",
    "    # Add actual data points  \n",
    "    wins = df[df['Win'] == 1][score_col]\n",
    "    losses = df[df['Win'] == 0][score_col]\n",
    "    \n",
    "    win_jitter = np.random.normal(1, 0.02, len(wins))\n",
    "    loss_jitter = np.random.normal(0, 0.02, len(losses))\n",
    "    \n",
    "    ax1.scatter(wins, win_jitter, color='green', alpha=0.7, s=50, label=f'Wins ({len(wins)})', zorder=5)\n",
    "    ax1.scatter(losses, loss_jitter, color='red', alpha=0.7, s=50, label=f'Losses ({len(losses)})', zorder=5)\n",
    "    \n",
    "    # Highlight actual data range\n",
    "    ax1.axvspan(df[score_col].min(), df[score_col].max(), alpha=0.15, color='yellow', label='Data Range')\n",
    "    \n",
    "    ax1.set_xlabel(f'{position_name} Rebalanced Score')\n",
    "    ax1.set_ylabel('Win Probability')\n",
    "    ax1.set_title(f'{position_name} - COMPLETE S-Curve (0% to 100%)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(-0.05, 1.05)\n",
    "    \n",
    "    # Plot 2: Distribution comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(losses, bins=15, alpha=0.7, color='red', label=f'Losses ({len(losses)})', density=True)\n",
    "    ax2.hist(wins, bins=15, alpha=0.7, color='green', label=f'Wins ({len(wins)})', density=True)\n",
    "    ax2.set_xlabel(f'{position_name} Rebalanced Score')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.set_title(f'{position_name} - Score Distribution')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    ax2.text(0.05, 0.95, f'Win Mean: {wins.mean():.1f}\\nLoss Mean: {losses.mean():.1f}\\nDiff: {wins.mean()-losses.mean():.1f}', \n",
    "             transform=ax2.transAxes, verticalalignment='top', \n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # Plot 3: Box plot\n",
    "    ax3 = axes[0, 2]\n",
    "    box_data = [losses, wins]\n",
    "    box_plot = ax3.boxplot(box_data, labels=['Losses', 'Wins'], patch_artist=True)\n",
    "    box_plot['boxes'][0].set_facecolor('red')\n",
    "    box_plot['boxes'][1].set_facecolor('green')\n",
    "    ax3.set_ylabel(f'{position_name} Rebalanced Score')\n",
    "    ax3.set_title(f'{position_name} - Score by Result')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: ROC CURVE\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    # Get predictions for ROC curve\n",
    "    X_test = df[[score_col]]\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_test = df['Win']\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    ax4 = axes[1, 0]\n",
    "    ax4.plot(fpr, tpr, color='darkorange', lw=3, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "    ax4.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    ax4.set_xlim([0.0, 1.0])\n",
    "    ax4.set_ylim([0.0, 1.05])\n",
    "    ax4.set_xlabel('False Positive Rate')\n",
    "    ax4.set_ylabel('True Positive Rate')\n",
    "    ax4.set_title('ROC Curve')\n",
    "    ax4.legend(loc=\"lower right\")\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: CONFUSION MATRIX\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    ax5 = axes[1, 1]\n",
    "    im = ax5.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax5.figure.colorbar(im, ax=ax5)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax5.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                    fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax5.set_ylabel('True Label')\n",
    "    ax5.set_xlabel('Predicted Label')\n",
    "    ax5.set_title('Confusion Matrix')\n",
    "    ax5.set_xticks([0, 1])\n",
    "    ax5.set_yticks([0, 1])\n",
    "    ax5.set_xticklabels(['Loss', 'Win'])\n",
    "    ax5.set_yticklabels(['Loss', 'Win'])\n",
    "    \n",
    "    # Add confusion matrix details\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    ax5.text(0.5, -0.15, f'True Negatives: {tn}\\nFalse Positives: {fp}\\nFalse Negatives: {fn}\\nTrue Positives: {tp}', \n",
    "             transform=ax5.transAxes, ha='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    # Plot 6: Performance Metrics\n",
    "    ax6 = axes[1, 2]\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    metrics_text = f\"\"\"\n",
    "    PERFORMANCE METRICS\n",
    "    ──────────────────────\n",
    "    Accuracy: {accuracy:.3f}\n",
    "    Precision: {precision:.3f}\n",
    "    Recall (Sensitivity): {recall:.3f}\n",
    "    Specificity: {specificity:.3f}\n",
    "    F1-Score: {f1_score:.3f}\n",
    "    AUC: {roc_auc:.3f}\n",
    "    \n",
    "    CONFUSION MATRIX BREAKDOWN\n",
    "    ──────────────────────────\n",
    "    True Positives (TP): {tp}\n",
    "    True Negatives (TN): {tn}\n",
    "    False Positives (FP): {fp}\n",
    "    False Negatives (FN): {fn}\n",
    "    \n",
    "    Total Predictions: {tp + tn + fp + fn}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax6.text(0.1, 0.9, metrics_text, transform=ax6.transAxes, fontsize=11,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed analysis\n",
    "    print(f\"\\n{position_name} DETAILED ANALYSIS:\")\n",
    "    print(f\"─\" * 50)\n",
    "    \n",
    "    # Find key thresholds\n",
    "    threshold_10 = threshold_50 = threshold_90 = None\n",
    "    for score, prob in zip(score_range.flatten(), win_probabilities):\n",
    "        if prob >= 0.1 and threshold_10 is None:\n",
    "            threshold_10 = score\n",
    "        if prob >= 0.5 and threshold_50 is None:\n",
    "            threshold_50 = score\n",
    "        if prob >= 0.9 and threshold_90 is None:\n",
    "            threshold_90 = score\n",
    "    \n",
    "    print(f\"📉 Score for 10% win prob: {threshold_10:.2f}\")\n",
    "    print(f\"🎯 Score for 50% win prob: {threshold_50:.2f}\")\n",
    "    print(f\"📈 Score for 90% win prob: {threshold_90:.2f}\")\n",
    "    \n",
    "    # Statistical test\n",
    "    t_stat, p_value = stats.ttest_ind(wins, losses)\n",
    "    print(f\"📊 T-test p-value: {p_value:.4f} ({'Significant' if p_value < 0.05 else 'Not significant'})\")\n",
    "    \n",
    "    print(f\"\\n🎯 CLASSIFICATION RESULTS:\")\n",
    "    print(f\"True Positives (Correctly predicted wins): {tp}\")\n",
    "    print(f\"True Negatives (Correctly predicted losses): {tn}\")\n",
    "    print(f\"False Positives (Predicted win, but lost): {fp}\")\n",
    "    print(f\"False Negatives (Predicted loss, but won): {fn}\")\n",
    "    \n",
    "    return {\n",
    "        'position': position_name,\n",
    "        'threshold_50': threshold_50,\n",
    "        'win_mean': wins.mean(),\n",
    "        'loss_mean': losses.mean(),\n",
    "        'p_value': p_value,\n",
    "        'coefficient': model.coef_[0][0],\n",
    "        'auc': roc_auc,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'confusion_matrix': {'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn}\n",
    "    }\n",
    "\n",
    "def analyze_all_positions_with_multiple_features(df):\n",
    "    \"\"\"Run analysis for Forward position only\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYZING FORWARD POSITION ONLY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = []\n",
    "    position = 'Forward'  # Only analyze Forward\n",
    "    \n",
    "    # Find all features for Forward position\n",
    "    pos_columns = [col for col in df.columns if col.startswith(f'{position}_') and ('_sum' in col or '_mean' in col)]\n",
    "    \n",
    "    if not pos_columns:\n",
    "        print(f\"❌ No features found for {position}\")\n",
    "        return results\n",
    "        \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ANALYZING {position.upper()} ({len(pos_columns)} features)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Show available features\n",
    "    sum_features = [col for col in pos_columns if '_sum' in col]\n",
    "    mean_features = [col for col in pos_columns if '_mean' in col]\n",
    "    \n",
    "    print(f\"📊 SUM features ({len(sum_features)}):\")\n",
    "    for feat in sum_features[:8]:  # Show first 8\n",
    "        print(f\"   • {feat}\")\n",
    "    if len(sum_features) > 8:\n",
    "        print(f\"   ... and {len(sum_features)-8} more\")\n",
    "        \n",
    "    print(f\"📊 MEAN features ({len(mean_features)}):\")\n",
    "    for feat in mean_features[:8]:  # Show first 8\n",
    "        print(f\"   • {feat}\")\n",
    "    if len(mean_features) > 8:\n",
    "        print(f\"   ... and {len(mean_features)-8} more\")\n",
    "    \n",
    "    # Create dataset for Forward position\n",
    "    analysis_columns = ['Date', 'Result', 'Win'] + pos_columns\n",
    "    pos_data = df[analysis_columns].copy()\n",
    "    pos_data = pos_data.dropna()\n",
    "    \n",
    "    if len(pos_data) < 20:\n",
    "        print(f\"⚠️ Skipping {position} - insufficient data ({len(pos_data)} matches)\")\n",
    "        return results\n",
    "    \n",
    "    print(f\"📈 Data: {len(pos_data)} matches\")\n",
    "    \n",
    "    # SINGLE FEATURE ANALYSIS (Rebalanced Score)\n",
    "    rebalanced_cols = [col for col in pos_columns if 'Rebalanced_Score' in col and '_sum' in col]\n",
    "    \n",
    "    if rebalanced_cols:\n",
    "        score_col = rebalanced_cols[0]\n",
    "        print(f\"\\n🎯 SINGLE FEATURE ANALYSIS: {score_col}\")\n",
    "        \n",
    "        X_single = pos_data[[score_col]]\n",
    "        y = pos_data['Win']\n",
    "        \n",
    "        # Train single feature model\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_single, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        scaler_single = StandardScaler()\n",
    "        X_train_scaled = scaler_single.fit_transform(X_train)\n",
    "        X_test_scaled = scaler_single.transform(X_test)\n",
    "        \n",
    "        model_single = LogisticRegression(random_state=42)\n",
    "        model_single.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred_proba_single = model_single.predict_proba(X_test_scaled)[:, 1]\n",
    "        auc_single = roc_auc_score(y_test, y_pred_proba_single)\n",
    "        \n",
    "        print(f\"   Single feature AUC: {auc_single:.3f}\")\n",
    "        \n",
    "        # Plot Forward analysis with S-curve, ROC, and confusion matrix\n",
    "        result_single = plot_forward_analysis_only(pos_data, score_col, model_single, scaler_single, f\"{position} (Rebalanced Score Only)\")\n",
    "        result_single['model_type'] = 'Single Feature'\n",
    "        results.append(result_single)\n",
    "    \n",
    "    # MULTIPLE FEATURES ANALYSIS\n",
    "    print(f\"\\n🔥 MULTIPLE FEATURES ANALYSIS: All {len(pos_columns)} features\")\n",
    "    \n",
    "    # Use all Forward features\n",
    "    X_multi = pos_data[pos_columns]\n",
    "    \n",
    "    # Remove features with zero variance\n",
    "    X_multi = X_multi.loc[:, X_multi.var() > 0.01]\n",
    "    \n",
    "    if X_multi.shape[1] < 2:\n",
    "        print(f\"⚠️ Insufficient features with variance for {position}\")\n",
    "        return results\n",
    "        \n",
    "    print(f\"   Using {X_multi.shape[1]} features after variance filtering\")\n",
    "    \n",
    "    # Train multiple features model\n",
    "    X_train_multi, X_test_multi, y_train, y_test = train_test_split(X_multi, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    scaler_multi = StandardScaler()\n",
    "    X_train_multi_scaled = scaler_multi.fit_transform(X_train_multi)\n",
    "    X_test_multi_scaled = scaler_multi.transform(X_test_multi)\n",
    "    \n",
    "    model_multi = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model_multi.fit(X_train_multi_scaled, y_train)\n",
    "    \n",
    "    y_pred_proba_multi = model_multi.predict_proba(X_test_multi_scaled)[:, 1]\n",
    "    auc_multi = roc_auc_score(y_test, y_pred_proba_multi)\n",
    "    \n",
    "    print(f\"   Multiple features AUC: {auc_multi:.3f}\")\n",
    "    \n",
    "    # Show feature importance for multi-feature model\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_multi.columns,\n",
    "        'Coefficient': model_multi.coef_[0],\n",
    "        'Abs_Coefficient': np.abs(model_multi.coef_[0])\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(f\"\\n🏆 TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "    for i, row in feature_importance.head(5).iterrows():\n",
    "        direction = \"↗️\" if row['Coefficient'] > 0 else \"↘️\"\n",
    "        print(f\"   {i+1}. {row['Feature']}: {row['Coefficient']:+.3f} {direction}\")\n",
    "    \n",
    "    # Get top feature for visualization\n",
    "    top_feature = feature_importance.iloc[0]['Feature']\n",
    "    \n",
    "    print(f\"\\n📊 Top feature identified: {top_feature}\")\n",
    "    \n",
    "    # Store results (no plotting)\n",
    "    result_multi = {\n",
    "        'position': position,\n",
    "        'model_type': 'Multiple Features',\n",
    "        'auc': auc_multi,\n",
    "        'num_features': X_multi.shape[1],\n",
    "        'top_feature': top_feature,\n",
    "        'top_coefficient': feature_importance.iloc[0]['Coefficient'],\n",
    "        'feature_importance': feature_importance.head(10).to_dict('records')\n",
    "    }\n",
    "    results.append(result_multi)\n",
    "    \n",
    "    # Compare single vs multiple\n",
    "    if rebalanced_cols:\n",
    "        improvement = auc_multi - auc_single\n",
    "        print(f\"\\n📊 MODEL COMPARISON:\")\n",
    "        print(f\"   Single Feature AUC: {auc_single:.3f}\")\n",
    "        print(f\"   Multiple Features AUC: {auc_multi:.3f}\")\n",
    "        print(f\"   Improvement: {improvement:+.3f} ({'Better' if improvement > 0.02 else 'Marginal' if improvement > 0 else 'Worse'})\")\n",
    "    \n",
    "    # Final summary for Forward only\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FORWARD POSITION ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if results:\n",
    "        single_results = [r for r in results if r.get('model_type') == 'Single Feature']\n",
    "        multi_results = [r for r in results if r.get('model_type') == 'Multiple Features']\n",
    "        \n",
    "        if single_results:\n",
    "            print(\"\\n🎯 SINGLE FEATURE MODEL (Rebalanced Score Only):\")\n",
    "            for r in single_results:\n",
    "                print(f\"   AUC: {r['auc']:.3f}\")\n",
    "        \n",
    "        if multi_results:\n",
    "            print(f\"\\n🔥 MULTIPLE FEATURES MODEL:\")\n",
    "            for r in multi_results:\n",
    "                print(f\"   AUC: {r['auc']:.3f} | Features: {r['num_features']} | Top: {r['top_feature'].split('_')[-1]}\")\n",
    "                \n",
    "        if single_results and multi_results:\n",
    "            improvement = multi_results[0]['auc'] - single_results[0]['auc']\n",
    "            status = \"🚀 Significant\" if improvement > 0.02 else \"📈 Marginal\" if improvement > 0 else \"📉 Worse\"\n",
    "            print(f\"\\n📈 MULTI-FEATURE BENEFIT: {improvement:+.3f} | {status}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution with comprehensive position-based analysis\"\"\"\n",
    "    \n",
    "    print(\"🚀 REAL MADRID COMPREHENSIVE POSITION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"📊 Features based on your rebalanced score formulas:\")\n",
    "    print(\"\\n📊 DEFENSE FEATURES:\")\n",
    "    print(\"   • Interceptions (Int) - Coefficient: 2.5\")\n",
    "    print(\"   • Blocks - Coefficient: 2.0\") \n",
    "    print(\"   • Clearances (Clr) - Coefficient: 1.0\")\n",
    "    print(\"   • Tackles Won (TklW) - Coefficient: 2.0\")\n",
    "    print(\"   • Defensive 3rd Tackles - Coefficient: 1.3\")\n",
    "    print(\"   • Middle 3rd Tackles - Coefficient: 0.8\")\n",
    "    \n",
    "    print(\"\\n📊 MIDFIELD FEATURES:\")\n",
    "    print(\"   • Pass Completion % - Coefficient: 2.5\")\n",
    "    print(\"   • Key Passes (KP) - Coefficient: 1.2\")\n",
    "    print(\"   • Tackles - Coefficient: 1.5\")\n",
    "    print(\"   • Progressive Carries - Coefficient: 0.8\")\n",
    "    print(\"   • Progressive Passes - Coefficient: 1.8\")\n",
    "    print(\"   • Touches - Coefficient: 0.3\")\n",
    "    \n",
    "    print(\"\\n📊 FORWARD FEATURES:\")\n",
    "    print(\"   • Goals (Gls) - Coefficient: 3.0\")\n",
    "    print(\"   • Assists (Ast) - Coefficient: 2.0\")\n",
    "    print(\"   • Shots on Target (SoT) - Coefficient: 1.0\")\n",
    "    print(\"   • Expected Goals (xG) - Coefficient: 1.5\")\n",
    "    print(\"   • Expected Assisted Goals (xAG) - Coefficient: 1.0\")\n",
    "    print(\"   • Take-ons - Coefficient: 0.5\")\n",
    "    \n",
    "    print(\"\\n📊 GOALKEEPER FEATURES:\")\n",
    "    print(\"   • Total Pass Completion % - Coefficient: 3.0\")\n",
    "    print(\"   • Errors - Coefficient: -2.0\")\n",
    "    print(\"   • Progressive Distance - Coefficient: 1.0\")\n",
    "    print(\"   • Short Pass Completion % - Coefficient: 1.5\")\n",
    "    print(\"   • Medium Pass Completion % - Coefficient: 1.0\")\n",
    "    print(\"   • Total Passes Completed - Coefficient: 0.5\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create position-based dataset with multiple features\n",
    "    df = create_position_based_dataset()\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"❌ Failed to create dataset\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n✅ Successfully created dataset with {len(df)} matches\")\n",
    "    \n",
    "    # Save dataset\n",
    "    output_path = \"/Users/mariamoramora/Documents/GitHub/ADS599_Capstone/Main Notebook/Data Folder/comprehensive_position_analysis.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"💾 Dataset saved to: {output_path}\")\n",
    "    \n",
    "    print(f\"\\n📋 Dataset overview:\")\n",
    "    print(f\"   • Total matches: {len(df)}\")\n",
    "    print(f\"   • Wins: {df['Win'].sum()}\")\n",
    "    print(f\"   • Losses: {len(df) - df['Win'].sum()}\")\n",
    "    print(f\"   • Win rate: {df['Win'].mean():.1%}\")\n",
    "    print(f\"   • Total features: {len(df.columns) - 3}\")  # Subtract Date, Result, Win\n",
    "    \n",
    "    # Run comprehensive analysis\n",
    "    results = analyze_all_positions_with_multiple_features(df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✅ COMPREHENSIVE ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"🎯 Single feature analysis (Rebalanced Score only)\")\n",
    "    print(\"🔥 Multiple features analysis (All position-specific metrics)\")\n",
    "    print(\"📈 Complete S-curves (0% to 100%) for each position\")\n",
    "    print(\"🏆 Feature importance rankings\")\n",
    "    print(\"📊 Model comparison (Single vs Multiple features)\")\n",
    "    print(\"💾 All data saved for further analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df, results\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    df, results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# EXTRACT AND PLOT ADJUSTED MODEL PERFORMANCE\n",
    "# ==========================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract performance data from your adjusted models\n",
    "def create_position_models_dict(adjusted_models):\n",
    "    \"\"\"Convert adjusted_models to the format expected by the plotting code\"\"\"\n",
    "    position_models = {}\n",
    "    \n",
    "    for position, model_info in adjusted_models.items():\n",
    "        # Create the expected structure\n",
    "        position_models[position] = {\n",
    "            'metrics': {\n",
    "                'train_r2': 0.0,  # Not available in your data\n",
    "                'test_r2': model_info['r2'],\n",
    "                'train_mae': 0.0,  # Not available in your data\n",
    "                'test_mae': model_info['mae']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return position_models\n",
    "\n",
    "# Convert your adjusted_models to the expected format\n",
    "if 'adjusted_models' in locals() and adjusted_models:\n",
    "    position_models = create_position_models_dict(adjusted_models)\n",
    "    print(\"✅ Created position_models dictionary from adjusted_models\")\n",
    "    print(f\"Available positions: {list(position_models.keys())}\")\n",
    "else:\n",
    "    print(\"❌ adjusted_models not found. Please run the adjusted weighted scoring code first.\")\n",
    "\n",
    "# ==========================================\n",
    "# PLOT ADJUSTED MODEL PERFORMANCE\n",
    "# ==========================================\n",
    "\n",
    "def plot_adjusted_model_performance(models_dict, title_prefix=\"Adjusted\"):\n",
    "    \"\"\"Plot model performance with proper formatting\"\"\"\n",
    "    \n",
    "    if not models_dict:\n",
    "        print(\"❌ No model data available for plotting\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📊 PLOTTING {title_prefix.upper()} MODEL PERFORMANCE BY POSITION\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Extract metrics\n",
    "    positions = list(models_dict.keys())\n",
    "    test_r2_scores = []\n",
    "    test_mae_scores = []\n",
    "    \n",
    "    for pos in positions:\n",
    "        test_r2_scores.append(models_dict[pos]['metrics']['test_r2'])\n",
    "        test_mae_scores.append(models_dict[pos]['metrics']['test_mae'])\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: R² Scores by Position\n",
    "    x = np.arange(len(positions))\n",
    "    width = 0.6\n",
    "    \n",
    "    bars1 = ax1.bar(x, test_r2_scores, width, alpha=0.8, color='steelblue')\n",
    "    \n",
    "    ax1.set_xlabel('Position', fontweight='bold', fontsize=12)\n",
    "    ax1.set_ylabel('R² Score', fontweight='bold', fontsize=12)\n",
    "    ax1.set_title(f'{title_prefix} Model Performance - R² by Position', fontweight='bold', fontsize=14)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(positions, rotation=45, ha='right')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    ax1.set_ylim(0, max(test_r2_scores) * 1.1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'{height:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: MAE Scores by Position\n",
    "    bars2 = ax2.bar(x, test_mae_scores, width, alpha=0.8, color='coral')\n",
    "    \n",
    "    ax2.set_xlabel('Position', fontweight='bold', fontsize=12)\n",
    "    ax2.set_ylabel('Mean Absolute Error', fontweight='bold', fontsize=12)\n",
    "    ax2.set_title(f'{title_prefix} Model Performance - MAE by Position', fontweight='bold', fontsize=14)\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(positions, rotation=45, ha='right')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax2.annotate(f'{height:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(f'{title_prefix} Model Performance Analysis by Position', \n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n📊 {title_prefix.upper()} MODEL SUMMARY:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Position':<15} {'Test R²':<10} {'Test MAE':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, pos in enumerate(positions):\n",
    "        print(f\"{pos:<15} {test_r2_scores[i]:<10.3f} {test_mae_scores[i]:<10.3f}\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_r2 = np.mean(test_r2_scores)\n",
    "    avg_mae = np.mean(test_mae_scores)\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Average':<15} {avg_r2:<10.3f} {avg_mae:<10.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'positions': positions,\n",
    "        'test_r2': test_r2_scores,\n",
    "        'test_mae': test_mae_scores,\n",
    "        'avg_r2': avg_r2,\n",
    "        'avg_mae': avg_mae\n",
    "    }\n",
    "\n",
    "# Plot the adjusted model performance\n",
    "if 'position_models' in locals():\n",
    "    adjusted_results = plot_adjusted_model_performance(position_models, \"Adjusted Weighted Score\")\n",
    "\n",
    "# ==========================================\n",
    "# COMPARISON WITH ORIGINAL (IF AVAILABLE)\n",
    "# ==========================================\n",
    "\n",
    "def compare_original_vs_adjusted(original_results, adjusted_results):\n",
    "    \"\"\"Compare original vs adjusted model performance\"\"\"\n",
    "    \n",
    "    if not original_results or not adjusted_results:\n",
    "        print(\"❌ Need both original and adjusted results for comparison\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"COMPARISON: ORIGINAL vs ADJUSTED WEIGHTED SCORING\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Find common positions\n",
    "    original_positions = set(original_results['positions'])\n",
    "    adjusted_positions = set(adjusted_results['positions'])\n",
    "    common_positions = list(original_positions.intersection(adjusted_positions))\n",
    "    \n",
    "    if not common_positions:\n",
    "        print(\"❌ No common positions found for comparison\")\n",
    "        return\n",
    "    \n",
    "    # Extract data for common positions\n",
    "    orig_r2 = []\n",
    "    adj_r2 = []\n",
    "    orig_mae = []\n",
    "    adj_mae = []\n",
    "    \n",
    "    for pos in common_positions:\n",
    "        orig_idx = original_results['positions'].index(pos)\n",
    "        adj_idx = adjusted_results['positions'].index(pos)\n",
    "        \n",
    "        orig_r2.append(original_results['test_r2'][orig_idx])\n",
    "        adj_r2.append(adjusted_results['test_r2'][adj_idx])\n",
    "        orig_mae.append(original_results['test_mae'][orig_idx])\n",
    "        adj_mae.append(adjusted_results['test_mae'][adj_idx])\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    x = np.arange(len(common_positions))\n",
    "    width = 0.35\n",
    "    \n",
    "    # R² Comparison\n",
    "    bars1 = ax1.bar(x - width/2, orig_r2, width, label='Original', alpha=0.8, color='lightblue')\n",
    "    bars2 = ax1.bar(x + width/2, adj_r2, width, label='Adjusted', alpha=0.8, color='darkblue')\n",
    "    \n",
    "    ax1.set_xlabel('Position', fontweight='bold')\n",
    "    ax1.set_ylabel('Test R² Score', fontweight='bold')\n",
    "    ax1.set_title('Model Comparison - Test R² by Position', fontweight='bold', fontsize=14)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(common_positions, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.annotate(f'{height:.3f}',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom',\n",
    "                        fontsize=9)\n",
    "    \n",
    "    # MAE Comparison\n",
    "    bars3 = ax2.bar(x - width/2, orig_mae, width, label='Original', alpha=0.8, color='lightcoral')\n",
    "    bars4 = ax2.bar(x + width/2, adj_mae, width, label='Adjusted', alpha=0.8, color='darkred')\n",
    "    \n",
    "    ax2.set_xlabel('Position', fontweight='bold')\n",
    "    ax2.set_ylabel('Test MAE', fontweight='bold')\n",
    "    ax2.set_title('Model Comparison - Test MAE by Position', fontweight='bold', fontsize=14)\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(common_positions, rotation=45, ha='right')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars3, bars4]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax2.annotate(f'{height:.3f}',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom',\n",
    "                        fontsize=9)\n",
    "    \n",
    "    plt.suptitle('Original vs Adjusted Weighted Score Models', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print comparison summary\n",
    "    print(f\"\\n📊 COMPARISON SUMMARY:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Position':<15} {'Orig R²':<10} {'Adj R²':<10} {'Orig MAE':<10} {'Adj MAE':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    for i, pos in enumerate(common_positions):\n",
    "        print(f\"{pos:<15} {orig_r2[i]:<10.3f} {adj_r2[i]:<10.3f} {orig_mae[i]:<10.3f} {adj_mae[i]:<10.3f}\")\n",
    "    \n",
    "    # Calculate improvements\n",
    "    avg_orig_r2 = np.mean(orig_r2)\n",
    "    avg_adj_r2 = np.mean(adj_r2)\n",
    "    avg_orig_mae = np.mean(orig_mae)\n",
    "    avg_adj_mae = np.mean(adj_mae)\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Average':<15} {avg_orig_r2:<10.3f} {avg_adj_r2:<10.3f} {avg_orig_mae:<10.3f} {avg_adj_mae:<10.3f}\")\n",
    "    \n",
    "    # Calculate percentage improvements\n",
    "    r2_improvement = ((avg_adj_r2 - avg_orig_r2) / avg_orig_r2) * 100\n",
    "    mae_improvement = ((avg_orig_mae - avg_adj_mae) / avg_orig_mae) * 100\n",
    "    \n",
    "    print(f\"\\n🎯 Adjusted Model Performance vs Original:\")\n",
    "    print(f\"   R² Score: {'↑' if r2_improvement > 0 else '↓'} {abs(r2_improvement):.1f}% {'improvement' if r2_improvement > 0 else 'decline'}\")\n",
    "    print(f\"   MAE:      {'↓' if mae_improvement > 0 else '↑'} {abs(mae_improvement):.1f}% {'improvement' if mae_improvement > 0 else 'decline'}\")\n",
    "\n",
    "# ==========================================\n",
    "# USAGE INSTRUCTIONS\n",
    "# ==========================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"USAGE INSTRUCTIONS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\n🔧 To use the original plotting code, you need:\")\n",
    "print(\"1. position_models dict with structure:\")\n",
    "print(\"   position_models['Position']['metrics']['test_r2']\")\n",
    "print(\"   position_models['Position']['metrics']['test_mae']\")\n",
    "\n",
    "print(\"\\n2. For XGBoost comparison, you need:\")\n",
    "print(\"   xgboost_models['Position']['model_info']['metrics']['test_r2']\")\n",
    "print(\"   xgboost_models['Position']['model_info']['metrics']['val_r2']\")\n",
    "\n",
    "print(\"\\n✅ Current status:\")\n",
    "if 'position_models' in locals():\n",
    "    print(f\"   position_models: Available with {len(position_models)} positions\")\n",
    "else:\n",
    "    print(\"   position_models: Not available\")\n",
    "\n",
    "if 'adjusted_models' in locals():\n",
    "    print(f\"   adjusted_models: Available with {len(adjusted_models)} positions\")\n",
    "else:\n",
    "    print(\"   adjusted_models: Not available\")\n",
    "\n",
    "print(f\"\\n🎯 Run this code after executing your adjusted weighted scoring analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c3190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PLOT THE R2 and MAE for each position\n",
    "# ==========================================    \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check if we have the position models from RandomForest\n",
    "if 'position_models' in locals() and position_models:\n",
    "\tprint(\"📊 PLOTTING RANDOM FOREST MODEL PERFORMANCE BY POSITION\")\n",
    "\tprint(\"-\" * 50)\n",
    "\t\n",
    "\t# Extract metrics for each position\n",
    "\tpositions = list(position_models.keys())\n",
    "\ttrain_r2_scores = []\n",
    "\ttest_r2_scores = []\n",
    "\ttrain_mae_scores = []\n",
    "\ttest_mae_scores = []\n",
    "\t\n",
    "\tfor pos in positions:\n",
    "\t\tmetrics = position_models[pos]['metrics']\n",
    "\t\ttrain_r2_scores.append(metrics['train_r2'])\n",
    "\t\ttest_r2_scores.append(metrics['test_r2'])\n",
    "\t\ttrain_mae_scores.append(metrics['train_mae'])\n",
    "\t\ttest_mae_scores.append(metrics['test_mae'])\n",
    "\t\n",
    "\t# Create figure with subplots\n",
    "\tfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\t\n",
    "\t# Plot 1: R² Scores by Position\n",
    "\tx = np.arange(len(positions))\n",
    "\twidth = 0.35\n",
    "\t\n",
    "\tbars1 = ax1.bar(x - width/2, train_r2_scores, width, label='Train R²', alpha=0.8)\n",
    "\tbars2 = ax1.bar(x + width/2, test_r2_scores, width, label='Test R²', alpha=0.8)\n",
    "\t\n",
    "\tax1.set_xlabel('Position', fontweight='bold')\n",
    "\tax1.set_ylabel('R² Score', fontweight='bold')\n",
    "\tax1.set_title('Random Forest Model Performance - R² by Position', fontweight='bold', fontsize=14)\n",
    "\tax1.set_xticks(x)\n",
    "\tax1.set_xticklabels(positions)\n",
    "\tax1.legend()\n",
    "\tax1.grid(True, alpha=0.3, axis='y')\n",
    "\tax1.set_ylim(0, 1.1)\n",
    "\t\n",
    "\t# Add value labels on bars\n",
    "\tfor bars in [bars1, bars2]:\n",
    "\t\tfor bar in bars:\n",
    "\t\t\theight = bar.get_height()\n",
    "\t\t\tax1.annotate(f'{height:.3f}',\n",
    "\t\t\t\t\t\txy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "\t\t\t\t\t\txytext=(0, 3),\n",
    "\t\t\t\t\t\ttextcoords=\"offset points\",\n",
    "\t\t\t\t\t\tha='center', va='bottom',\n",
    "\t\t\t\t\t\tfontsize=9)\n",
    "\t\n",
    "\t# Plot 2: MAE Scores by Position\n",
    "\tbars3 = ax2.bar(x - width/2, train_mae_scores, width, label='Train MAE', alpha=0.8, color='orange')\n",
    "\tbars4 = ax2.bar(x + width/2, test_mae_scores, width, label='Test MAE', alpha=0.8, color='red')\n",
    "\t\n",
    "\tax2.set_xlabel('Position', fontweight='bold')\n",
    "\tax2.set_ylabel('Mean Absolute Error', fontweight='bold')\n",
    "\tax2.set_title('Random Forest Model Performance - MAE by Position', fontweight='bold', fontsize=14)\n",
    "\tax2.set_xticks(x)\n",
    "\tax2.set_xticklabels(positions)\n",
    "\tax2.legend()\n",
    "\tax2.grid(True, alpha=0.3, axis='y')\n",
    "\t\n",
    "\t# Add value labels on bars\n",
    "\tfor bars in [bars3, bars4]:\n",
    "\t\tfor bar in bars:\n",
    "\t\t\theight = bar.get_height()\n",
    "\t\t\tax2.annotate(f'{height:.2f}',\n",
    "\t\t\t\t\t\txy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "\t\t\t\t\t\txytext=(0, 3),\n",
    "\t\t\t\t\t\ttextcoords=\"offset points\",\n",
    "\t\t\t\t\t\tha='center', va='bottom',\n",
    "\t\t\t\t\t\tfontsize=9)\n",
    "\t\n",
    "\tplt.suptitle('Random Forest Model Performance Analysis by Position', fontsize=16, fontweight='bold', y=1.02)\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\t\n",
    "\t# Print summary statistics\n",
    "\tprint(\"\\n📊 RANDOM FOREST MODEL SUMMARY:\")\n",
    "\tprint(\"-\" * 60)\n",
    "\tprint(f\"{'Position':<15} {'Train R²':<10} {'Test R²':<10} {'Train MAE':<10} {'Test MAE':<10}\")\n",
    "\tprint(\"-\" * 60)\n",
    "\tfor i, pos in enumerate(positions):\n",
    "\t\tprint(f\"{pos:<15} {train_r2_scores[i]:<10.3f} {test_r2_scores[i]:<10.3f} {train_mae_scores[i]:<10.2f} {test_mae_scores[i]:<10.2f}\")\n",
    "\t\n",
    "\t# Calculate average performance\n",
    "\tavg_test_r2 = np.mean(test_r2_scores)\n",
    "\tavg_test_mae = np.mean(test_mae_scores)\n",
    "\tprint(\"-\" * 60)\n",
    "\tprint(f\"{'Average':<15} {np.mean(train_r2_scores):<10.3f} {avg_test_r2:<10.3f} {np.mean(train_mae_scores):<10.2f} {avg_test_mae:<10.2f}\")\n",
    "\n",
    "# Check if we have XGBoost models\n",
    "if 'xgboost_models' in locals() and xgboost_models:\n",
    "\tprint(\"\\n\\n📊 PLOTTING XGBOOST MODEL PERFORMANCE BY POSITION\")\n",
    "\tprint(\"-\" * 50)\n",
    "\t\n",
    "\t# Extract metrics for each position\n",
    "\txgb_positions = list(xgboost_models.keys())\n",
    "\txgb_test_r2_scores = []\n",
    "\txgb_test_mae_scores = []\n",
    "\txgb_val_r2_scores = []\n",
    "\txgb_val_mae_scores = []\n",
    "\t\n",
    "\tfor pos in xgb_positions:\n",
    "\t\tmetrics = xgboost_models[pos]['model_info']['metrics']\n",
    "\t\txgb_test_r2_scores.append(metrics['test_r2'])\n",
    "\t\txgb_test_mae_scores.append(metrics['test_mae'])\n",
    "\t\txgb_val_r2_scores.append(metrics['val_r2'])\n",
    "\t\txgb_val_mae_scores.append(metrics['val_mae'])\n",
    "\t\n",
    "\t# Create figure with subplots\n",
    "\tfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\t\n",
    "\t# Plot 1: R² Scores by Position\n",
    "\tx = np.arange(len(xgb_positions))\n",
    "\twidth = 0.35\n",
    "\t\n",
    "\tbars1 = ax1.bar(x - width/2, xgb_val_r2_scores, width, label='Validation R²', alpha=0.8, color='lightgreen')\n",
    "\tbars2 = ax1.bar(x + width/2, xgb_test_r2_scores, width, label='Test R²', alpha=0.8, color='darkgreen')\n",
    "\t\n",
    "\tax1.set_xlabel('Model', fontweight='bold')\n",
    "\tax1.set_ylabel('R² Score', fontweight='bold')\n",
    "\tax1.set_title('XGBoost Model Performance - R² by Position', fontweight='bold', fontsize=14)\n",
    "\tax1.set_xticks(x)\n",
    "\tax1.set_xticklabels(xgb_positions)\n",
    "\tax1.legend()\n",
    "\tax1.grid(True, alpha=0.3, axis='y')\n",
    "\tax1.set_ylim(0.8, 1.02)\n",
    "\t\n",
    "\t# Add value labels on bars\n",
    "\tfor bars in [bars1, bars2]:\n",
    "\t\tfor bar in bars:\n",
    "\t\t\theight = bar.get_height()\n",
    "\t\t\tax1.annotate(f'{height:.3f}',\n",
    "\t\t\t\t\t\txy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "\t\t\t\t\t\txytext=(0, 3),\n",
    "\t\t\t\t\t\ttextcoords=\"offset points\",\n",
    "\t\t\t\t\t\tha='center', va='bottom',\n",
    "\t\t\t\t\t\tfontsize=9)\n",
    "\t\n",
    "\t# Plot 2: MAE Scores by Position\n",
    "\tbars3 = ax2.bar(x - width/2, xgb_val_mae_scores, width, label='Validation MAE', alpha=0.8, color='lightcoral')\n",
    "\tbars4 = ax2.bar(x + width/2, xgb_test_mae_scores, width, label='Test MAE', alpha=0.8, color='darkred')\n",
    "\t\n",
    "\tax2.set_xlabel('Model', fontweight='bold')\n",
    "\tax2.set_ylabel('Mean Absolute Error', fontweight='bold')\n",
    "\tax2.set_title('XGBoost Model Performance - MAE by Position', fontweight='bold', fontsize=14)\n",
    "\tax2.set_xticks(x)\n",
    "\tax2.set_xticklabels(xgb_positions)\n",
    "\tax2.legend()\n",
    "\tax2.grid(True, alpha=0.3, axis='y')\n",
    "\t\n",
    "\t# Add value labels on bars\n",
    "\tfor bars in [bars3, bars4]:\n",
    "\t\tfor bar in bars:\n",
    "\t\t\theight = bar.get_height()\n",
    "\t\t\tax2.annotate(f'{height:.2f}',\n",
    "\t\t\t\t\t\txy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "\t\t\t\t\t\txytext=(0, 3),\n",
    "\t\t\t\t\t\ttextcoords=\"offset points\",\n",
    "\t\t\t\t\t\tha='center', va='bottom',\n",
    "\t\t\t\t\t\tfontsize=9)\n",
    "\t\n",
    "\tplt.suptitle('XGBoost Model Performance Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\t\n",
    "\t# Print summary statistics\n",
    "\tprint(\"\\n📊 XGBOOST MODEL SUMMARY:\")\n",
    "\tprint(\"-\" * 70)\n",
    "\tprint(f\"{'Model':<15} {'Val R²':<10} {'Test R²':<10} {'Val MAE':<10} {'Test MAE':<10}\")\n",
    "\tprint(\"-\" * 70)\n",
    "\tfor i, pos in enumerate(xgb_positions):\n",
    "\t\tprint(f\"{pos:<15} {xgb_val_r2_scores[i]:<10.3f} {xgb_test_r2_scores[i]:<10.3f} {xgb_val_mae_scores[i]:<10.2f} {xgb_test_mae_scores[i]:<10.2f}\")\n",
    "\t\n",
    "\t# Calculate average performance\n",
    "\tavg_test_r2 = np.mean(xgb_test_r2_scores)\n",
    "\tavg_test_mae = np.mean(xgb_test_mae_scores)\n",
    "\tprint(\"-\" * 70)\n",
    "\tprint(f\"{'Average':<15} {np.mean(xgb_val_r2_scores):<10.3f} {avg_test_r2:<10.3f} {np.mean(xgb_val_mae_scores):<10.2f} {avg_test_mae:<10.2f}\")\n",
    "\n",
    "# Comparison plot between RandomForest and XGBoost (if both exist)\n",
    "if 'position_models' in locals() and 'xgboost_models' in locals() and position_models and xgboost_models:\n",
    "\tprint(\"\\n\\n📊 COMPARING RANDOM FOREST VS XGBOOST PERFORMANCE\")\n",
    "\tprint(\"-\" * 50)\n",
    "\t\n",
    "\t# Find common positions\n",
    "\trf_positions = set(position_models.keys())\n",
    "\txgb_positions = set(xgboost_models.keys()) - {'Combined'}  # Exclude combined model\n",
    "\tcommon_positions = list(rf_positions.intersection(xgb_positions))\n",
    "\t\n",
    "\tif common_positions:\n",
    "\t\t# Extract test R² scores for comparison\n",
    "\t\trf_test_r2 = [position_models[pos]['metrics']['test_r2'] for pos in common_positions]\n",
    "\t\txgb_test_r2 = [xgboost_models[pos]['model_info']['metrics']['test_r2'] for pos in common_positions]\n",
    "\t\t\n",
    "\t\trf_test_mae = [position_models[pos]['metrics']['test_mae'] for pos in common_positions]\n",
    "\t\txgb_test_mae = [xgboost_models[pos]['model_info']['metrics']['test_mae'] for pos in common_positions]\n",
    "\t\t\n",
    "\t\t# Create comparison plot\n",
    "\t\tfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\t\t\n",
    "\t\t# R² Comparison\n",
    "\t\tx = np.arange(len(common_positions))\n",
    "\t\twidth = 0.35\n",
    "\t\t\n",
    "\t\tbars1 = ax1.bar(x - width/2, rf_test_r2, width, label='Random Forest', alpha=0.8, color='steelblue')\n",
    "\t\tbars2 = ax1.bar(x + width/2, xgb_test_r2, width, label='XGBoost', alpha=0.8, color='darkgreen')\n",
    "\t\t\n",
    "\t\tax1.set_xlabel('Position', fontweight='bold')\n",
    "\t\tax1.set_ylabel('Test R² Score', fontweight='bold')\n",
    "\t\tax1.set_title('Model Comparison - Test R² by Position', fontweight='bold', fontsize=14)\n",
    "\t\tax1.set_xticks(x)\n",
    "\t\tax1.set_xticklabels(common_positions)\n",
    "\t\tax1.legend()\n",
    "\t\tax1.grid(True, alpha=0.3, axis='y')\n",
    "\t\tax1.set_ylim(0.8, 1.02)\n",
    "\t\t\n",
    "\t\t# Add value labels\n",
    "\t\tfor bars in [bars1, bars2]:\n",
    "\t\t\tfor bar in bars:\n",
    "\t\t\t\theight = bar.get_height()\n",
    "\t\t\t\tax1.annotate(f'{height:.3f}',\n",
    "\t\t\t\t\t\t\txy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "\t\t\t\t\t\t\txytext=(0, 3),\n",
    "\t\t\t\t\t\t\ttextcoords=\"offset points\",\n",
    "\t\t\t\t\t\t\tha='center', va='bottom',\n",
    "\t\t\t\t\t\t\tfontsize=9)\n",
    "\t\t\n",
    "\t\t# MAE Comparison\n",
    "\t\tbars3 = ax2.bar(x - width/2, rf_test_mae, width, label='Random Forest', alpha=0.8, color='steelblue')\n",
    "\t\tbars4 = ax2.bar(x + width/2, xgb_test_mae, width, label='XGBoost', alpha=0.8, color='darkgreen')\n",
    "\t\t\n",
    "\t\tax2.set_xlabel('Position', fontweight='bold')\n",
    "\t\tax2.set_ylabel('Test MAE', fontweight='bold')\n",
    "\t\tax2.set_title('Model Comparison - Test MAE by Position', fontweight='bold', fontsize=14)\n",
    "\t\tax2.set_xticks(x)\n",
    "\t\tax2.set_xticklabels(common_positions)\n",
    "\t\tax2.legend()\n",
    "\t\tax2.grid(True, alpha=0.3, axis='y')\n",
    "\t\t\n",
    "\t\t# Add value labels\n",
    "\t\tfor bars in [bars3, bars4]:\n",
    "\t\t\tfor bar in bars:\n",
    "\t\t\t\theight = bar.get_height()\n",
    "\t\t\t\tax2.annotate(f'{height:.2f}',\n",
    "\t\t\t\t\t\t\txy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "\t\t\t\t\t\t\txytext=(0, 3),\n",
    "\t\t\t\t\t\t\ttextcoords=\"offset points\",\n",
    "\t\t\t\t\t\t\tha='center', va='bottom',\n",
    "\t\t\t\t\t\t\tfontsize=9)\n",
    "\t\t\n",
    "\t\tplt.suptitle('Random Forest vs XGBoost Model Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.show()\n",
    "\t\t\n",
    "\t\t# Print comparison summary\n",
    "\t\tprint(\"\\n📊 MODEL COMPARISON SUMMARY:\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\t\tprint(f\"{'Position':<15} {'RF Test R²':<12} {'XGB Test R²':<12} {'RF Test MAE':<12} {'XGB Test MAE':<12}\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\t\tfor i, pos in enumerate(common_positions):\n",
    "\t\t\tprint(f\"{pos:<15} {rf_test_r2[i]:<12.3f} {xgb_test_r2[i]:<12.3f} {rf_test_mae[i]:<12.2f} {xgb_test_mae[i]:<12.2f}\")\n",
    "\t\t\n",
    "\t\t# Calculate averages and improvements\n",
    "\t\tavg_rf_r2 = np.mean(rf_test_r2)\n",
    "\t\tavg_xgb_r2 = np.mean(xgb_test_r2)\n",
    "\t\tavg_rf_mae = np.mean(rf_test_mae)\n",
    "\t\tavg_xgb_mae = np.mean(xgb_test_mae)\n",
    "\t\t\n",
    "\t\tprint(\"-\" * 80)\n",
    "\t\tprint(f\"{'Average':<15} {avg_rf_r2:<12.3f} {avg_xgb_r2:<12.3f} {avg_rf_mae:<12.2f} {avg_xgb_mae:<12.2f}\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\t\t\n",
    "\t\t# Calculate improvements\n",
    "\t\tr2_improvement = ((avg_xgb_r2 - avg_rf_r2) / avg_rf_r2) * 100\n",
    "\t\tmae_improvement = ((avg_rf_mae - avg_xgb_mae) / avg_rf_mae) * 100\n",
    "\t\t\n",
    "\t\tprint(f\"\\n🎯 XGBoost Performance vs Random Forest:\")\n",
    "\t\tprint(f\"   R² Score: {'↑' if r2_improvement > 0 else '↓'} {abs(r2_improvement):.1f}% {'better' if r2_improvement > 0 else 'worse'}\")\n",
    "\t\tprint(f\"   MAE:      {'↓' if mae_improvement > 0 else '↑'} {abs(mae_improvement):.1f}% {'better' if mae_improvement > 0 else 'worse'}\")\n",
    "\t\t\n",
    "\t\t# Determine overall winner\n",
    "\t\tif r2_improvement > 0 and mae_improvement > 0:\n",
    "\t\t\tprint(f\"\\n🏆 Winner: XGBoost (better on both metrics)\")\n",
    "\t\telif r2_improvement < 0 and mae_improvement < 0:\n",
    "\t\t\tprint(f\"\\n🏆 Winner: Random Forest (better on both metrics)\")\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"\\n🏆 Winner: Mixed results - depends on metric priority\")\n",
    "\n",
    "else:\n",
    "\tprint(\"⚠️ No model data available for plotting. Please ensure position_models and/or xgboost_models are defined.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check if we have the position models from RandomForest\n",
    "if 'position_models' in locals() and position_models:\n",
    "\tprint(\"📊 PLOTTING RANDOM FOREST MODEL PERFORMANCE BY POSITION\")\n",
    "\tprint(\"-\" * 50)\n",
    "\t\n",
    "\t# Extract metrics for each position\n",
    "\tpositions = list(position_models.keys())\n",
    "\ttrain_r2_scores = []\n",
    "\ttest_r2_scores = []\n",
    "\ttrain_mae_scores = []\n",
    "\ttest_mae_scores = []\n",
    "\t\n",
    "\tfor pos in positions:\n",
    "\t\tmetrics = position_models[pos]['metrics']\n",
    "\t\ttrain_r2_scores.append(metrics['train_r2'])\n",
    "\t\ttest_r2_scores.append(metrics['test_r2'])\n",
    "\t\ttrain_mae_scores.append(metrics['train_mae'])\n",
    "\t\ttest_mae_scores.append(metrics['test_mae'])\n",
    "\t\n",
    "\t# Create figure with subplots\n",
    "\tfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\t\n",
    "\t# Plot 1: R² Scores by Position\n",
    "\tx = np.arange(len(positions))\n",
    "\twidth = 0.35\n",
    "\t\n",
    "\tbars1 = ax1.bar(x - width/2, train_r2_scores, width, label='Train R²', alpha=0.8)\n",
    "\tbars2 = ax1.bar(x + width/2, test_r2_scores, width, label='Test R²', alpha=0.8)\n",
    "\t\n",
    "\tax1.set_xlabel('Position', fontweight='bold')\n",
    "\tax1.set_ylabel('R² Score', fontweight='bold')\n",
    "\tax1.set_title('Random Forest Model Performance - R² by Position', fontweight='bold', fontsize=14)\n",
    "\tax1.set_xticks(x)\n",
    "\tax1.set_xticklabels(positions)\n",
    "\tax1.legend()\n",
    "\tax1.grid(True, alpha=0.3, axis='y')\n",
    "\tax1.set_ylim(0, 1.1)\n",
    "\t\n",
    "\t# Add value labels on bars\n",
    "\tfor bars in [bars1, bars2]:\n",
    "\t\tfor bar in bars:\n",
    "\t\t\theight = bar.get_height()\n",
    "\t\t\tax1.annotate(f'{height:.3f}',\n",
    "\t\t\t\t\t\txy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "\t\t\t\t\t\txytext=(0, 3),\n",
    "\t\t\t\t\t\ttextcoords=\"offset points\",\n",
    "\t\t\t\t\t\tha='center', va='bottom',\n",
    "\t\t\t\t\t\tfontsize=9)\n",
    "\t\n",
    "\t# Plot 2: MAE Scores by Position\n",
    "\tbars3 = ax2.bar(x - width/2, train_mae_scores, width, label='Train MAE', alpha=0.8, color='orange')\n",
    "\tbars4 = ax2.bar(x + width/2, test_mae_scores, width, label='Test MAE', alpha=0.8, color='red')\n",
    "\t\n",
    "\tax2.set_xlabel('Position', fontweight='bold')\n",
    "\tax2.set_ylabel('Mean Absolute Error', fontweight='bold')\n",
    "\tax2.set_title('Random Forest Model Performance - MAE by Position', fontweight='bold', fontsize=14)\n",
    "\tax2.set_xticks(x)\n",
    "\tax2.set_xticklabels(positions)\n",
    "\tax2.legend()\n",
    "\tax2.grid(True, alpha=0.3, axis='y')\n",
    "\t\n",
    "\t# Add value labels on bars\n",
    "\tfor bars in [bars3, bars4]:\n",
    "\t\tfor bar in bars:\n",
    "\t\t\theight = bar.get_height()\n",
    "\t\t\tax2.annotate(f'{height:.2f}',\n",
    "\t\t\t\t\t\txy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "\t\t\t\t\t\txytext=(0, 3),\n",
    "\t\t\t\t\t\ttextcoords=\"offset points\",\n",
    "\t\t\t\t\t\tha='center', va='bottom',\n",
    "\t\t\t\t\t\tfontsize=9)\n",
    "\t\n",
    "\tplt.suptitle('Random Forest Model Performance Analysis by Position', fontsize=16, fontweight='bold', y=1.02)\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\t\n",
    "\t# Print summary statistics\n",
    "\tprint(\"\\n📊 RANDOM FOREST MODEL SUMMARY:\")\n",
    "\tprint(\"-\" * 60)\n",
    "\tprint(f\"{'Position':<15} {'Train R²':<10} {'Test R²':<10} {'Train MAE':<10} {'Test MAE':<10}\")\n",
    "\tprint(\"-\" * 60)\n",
    "\tfor i, pos in enumerate(positions):\n",
    "\t\tprint(f\"{pos:<15} {train_r2_scores[i]:<10.3f} {test_r2_scores[i]:<10.3f} {train_mae_scores[i]:<10.2f} {test_mae_scores[i]:<10.2f}\")\n",
    "\t\n",
    "\t# Calculate average performance\n",
    "\tavg_test_r2 = np.mean(test_r2_scores)\n",
    "\tavg_test_mae = np.mean(test_mae_scores)\n",
    "\tprint(\"-\" * 60)\n",
    "\tprint(f\"{'Average':<15} {np.mean(train_r2_scores):<10.3f} {avg_test_r2:<10.3f} {np.mean(train_mae_scores):<10.2f} {avg_test_mae:<10.2f}\")\n",
    "\n",
    "# Check if we have XGBoost models\n",
    "if 'xgboost_models' in locals() and xgboost_models:\n",
    "\tprint(\"\\n\\n📊 PLOTTING XGBOOST MODEL PERFORMANCE BY POSITION\")\n",
    "\tprint(\"-\" * 50)\n",
    "\t\n",
    "\t# Extract metrics for each position\n",
    "\txgb_positions = list(xgboost_models.keys())\n",
    "\txgb_test_r2_scores = []\n",
    "\txgb_test_mae_scores = []\n",
    "\txgb_val_r2_scores = []\n",
    "\txgb_val_mae_scores = []\n",
    "\t\n",
    "\tfor pos in xgb_positions:\n",
    "\t\tmetrics = xgboost_models[pos]['model_info']['metrics']\n",
    "\t\txgb_test_r2_scores.append(metrics['test_r2'])\n",
    "\t\txgb_test_mae_scores.append(metrics['test_mae'])\n",
    "\t\txgb_val_r2_scores.append(metrics['val_r2'])\n",
    "\t\txgb_val_mae_scores.append(metrics['val_mae'])\n",
    "\t\n",
    "\t# Create figure with subplots\n",
    "\tfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\t\n",
    "\t# Plot 1: R² Scores by Position\n",
    "\tx = np.arange(len(xgb_positions))\n",
    "\twidth = 0.35\n",
    "\t\n",
    "\tbars1 = ax1.bar(x - width/2, xgb_val_r2_scores, width, label='Validation R²', alpha=0.8, color='lightgreen')\n",
    "\tbars2 = ax1.bar(x + width/2, xgb_test_r2_scores, width, label='Test R²', alpha=0.8, color='darkgreen')\n",
    "\t\n",
    "\tax1.set_xlabel('Model', fontweight='bold')\n",
    "\tax1.set_ylabel('R² Score', fontweight='bold')\n",
    "\tax1.set_title('XGBoost Model Performance - R² by Position', fontweight='bold', fontsize=14)\n",
    "\tax1.set_xticks(x)\n",
    "\tax1.set_xticklabels(xgb_positions)\n",
    "\tax1.legend()\n",
    "\tax1.grid(True, alpha=0.3, axis='y')\n",
    "\tax1.set_ylim(0.8, 1.02)\n",
    "\t\n",
    "\t# Add value labels on bars\n",
    "\tfor bars in [bars1, bars2]:\n",
    "\t\tfor bar in bars:\n",
    "\t\t\theight = bar.get_height()\n",
    "\t\t\tax1.annotate(f'{height:.3f}',\n",
    "\t\t\t\t\t\txy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "\t\t\t\t\t\txytext=(0, 3),\n",
    "\t\t\t\t\t\ttextcoords=\"offset points\",\n",
    "\t\t\t\t\t\tha='center', va='bottom',\n",
    "\t\t\t\t\t\tfontsize=9)\n",
    "\t\n",
    "\t# Plot 2: MAE Scores by Position\n",
    "\tbars3 = ax2.bar(x - width/2, xgb_val_mae_scores, width, label='Validation MAE', alpha=0.8, color='lightcoral')\n",
    "\tbars4 = ax2.bar(x + width/2, xgb_test_mae_scores, width, label='Test MAE', alpha=0.8, color='darkred')\n",
    "\t\n",
    "\tax2.set_xlabel('Model', fontweight='bold')\n",
    "\tax2.set_ylabel('Mean Absolute Error', fontweight='bold')\n",
    "\tax2.set_title('XGBoost Model Performance - MAE by Position', fontweight='bold', fontsize=14)\n",
    "\tax2.set_xticks(x)\n",
    "\tax2.set_xticklabels(xgb_positions)\n",
    "\tax2.legend()\n",
    "\tax2.grid(True, alpha=0.3, axis='y')\n",
    "\t\n",
    "\t# Add value labels on bars\n",
    "\tfor bars in [bars3, bars4]:\n",
    "\t\tfor bar in bars:\n",
    "\t\t\theight = bar.get_height()\n",
    "\t\t\tax2.annotate(f'{height:.2f}',\n",
    "\t\t\t\t\t\txy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "\t\t\t\t\t\txytext=(0, 3),\n",
    "\t\t\t\t\t\ttextcoords=\"offset points\",\n",
    "\t\t\t\t\t\tha='center', va='bottom',\n",
    "\t\t\t\t\t\tfontsize=9)\n",
    "\t\n",
    "\tplt.suptitle('XGBoost Model Performance Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\t\n",
    "\t# Print summary statistics\n",
    "\tprint(\"\\n📊 XGBOOST MODEL SUMMARY:\")\n",
    "\tprint(\"-\" * 70)\n",
    "\tprint(f\"{'Model':<15} {'Val R²':<10} {'Test R²':<10} {'Val MAE':<10} {'Test MAE':<10}\")\n",
    "\tprint(\"-\" * 70)\n",
    "\tfor i, pos in enumerate(xgb_positions):\n",
    "\t\tprint(f\"{pos:<15} {xgb_val_r2_scores[i]:<10.3f} {xgb_test_r2_scores[i]:<10.3f} {xgb_val_mae_scores[i]:<10.2f} {xgb_test_mae_scores[i]:<10.2f}\")\n",
    "\t\n",
    "\t# Calculate average performance\n",
    "\tavg_test_r2 = np.mean(xgb_test_r2_scores)\n",
    "\tavg_test_mae = np.mean(xgb_test_mae_scores)\n",
    "\tprint(\"-\" * 70)\n",
    "\tprint(f\"{'Average':<15} {np.mean(xgb_val_r2_scores):<10.3f} {avg_test_r2:<10.3f} {np.mean(xgb_val_mae_scores):<10.2f} {avg_test_mae:<10.2f}\")\n",
    "\n",
    "# Comparison plot between RandomForest and XGBoost (if both exist)\n",
    "if 'position_models' in locals() and 'xgboost_models' in locals() and position_models and xgboost_models:\n",
    "\tprint(\"\\n\\n📊 COMPARING RANDOM FOREST VS XGBOOST PERFORMANCE\")\n",
    "\tprint(\"-\" * 50)\n",
    "\t\n",
    "\t# Find common positions\n",
    "\trf_positions = set(position_models.keys())\n",
    "\txgb_positions = set(xgboost_models.keys()) - {'Combined'}  # Exclude combined model\n",
    "\tcommon_positions = list(rf_positions.intersection(xgb_positions))\n",
    "\t\n",
    "\tif common_positions:\n",
    "\t\t# Extract test R² scores for comparison\n",
    "\t\trf_test_r2 = [position_models[pos]['metrics']['test_r2'] for pos in common_positions]\n",
    "\t\txgb_test_r2 = [xgboost_models[pos]['model_info']['metrics']['test_r2'] for pos in common_positions]\n",
    "\t\t\n",
    "\t\trf_test_mae = [position_models[pos]['metrics']['test_mae'] for pos in common_positions]\n",
    "\t\txgb_test_mae = [xgboost_models[pos]['model_info']['metrics']['test_mae'] for pos in common_positions]\n",
    "\t\t\n",
    "\t\t# Create comparison plot\n",
    "\t\tfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\t\t\n",
    "\t\t# R² Comparison\n",
    "\t\tx = np.arange(len(common_positions))\n",
    "\t\twidth = 0.35\n",
    "\t\t\n",
    "\t\tbars1 = ax1.bar(x - width/2, rf_test_r2, width, label='Random Forest', alpha=0.8, color='steelblue')\n",
    "\t\tbars2 = ax1.bar(x + width/2, xgb_test_r2, width, label='XGBoost', alpha=0.8, color='darkgreen')\n",
    "\t\t\n",
    "\t\tax1.set_xlabel('Position', fontweight='bold')\n",
    "\t\tax1.set_ylabel('Test R² Score', fontweight='bold')\n",
    "\t\tax1.set_title('Model Comparison - Test R² by Position', fontweight='bold', fontsize=14)\n",
    "\t\tax1.set_xticks(x)\n",
    "\t\tax1.set_xticklabels(common_positions)\n",
    "\t\tax1.legend()\n",
    "\t\tax1.grid(True, alpha=0.3, axis='y')\n",
    "\t\tax1.set_ylim(0.8, 1.02)\n",
    "\t\t\n",
    "\t\t# Add value labels\n",
    "\t\tfor bars in [bars1, bars2]:\n",
    "\t\t\tfor bar in bars:\n",
    "\t\t\t\theight = bar.get_height()\n",
    "\t\t\t\tax1.annotate(f'{height:.3f}',\n",
    "\t\t\t\t\t\t\txy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "\t\t\t\t\t\t\txytext=(0, 3),\n",
    "\t\t\t\t\t\t\ttextcoords=\"offset points\",\n",
    "\t\t\t\t\t\t\tha='center', va='bottom',\n",
    "\t\t\t\t\t\t\tfontsize=9)\n",
    "\t\t\n",
    "\t\t# MAE Comparison\n",
    "\t\tbars3 = ax2.bar(x - width/2, rf_test_mae, width, label='Random Forest', alpha=0.8, color='steelblue')\n",
    "\t\tbars4 = ax2.bar(x + width/2, xgb_test_mae, width, label='XGBoost', alpha=0.8, color='darkgreen')\n",
    "\t\t\n",
    "\t\tax2.set_xlabel('Position', fontweight='bold')\n",
    "\t\tax2.set_ylabel('Test MAE', fontweight='bold')\n",
    "\t\tax2.set_title('Model Comparison - Test MAE by Position', fontweight='bold', fontsize=14)\n",
    "\t\tax2.set_xticks(x)\n",
    "\t\tax2.set_xticklabels(common_positions)\n",
    "\t\tax2.legend()\n",
    "\t\tax2.grid(True, alpha=0.3, axis='y')\n",
    "\t\t\n",
    "\t\t# Add value labels\n",
    "\t\tfor bars in [bars3, bars4]:\n",
    "\t\t\tfor bar in bars:\n",
    "\t\t\t\theight = bar.get_height()\n",
    "\t\t\t\tax2.annotate(f'{height:.2f}',\n",
    "\t\t\t\t\t\t\txy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "\t\t\t\t\t\t\txytext=(0, 3),\n",
    "\t\t\t\t\t\t\ttextcoords=\"offset points\",\n",
    "\t\t\t\t\t\t\tha='center', va='bottom',\n",
    "\t\t\t\t\t\t\tfontsize=9)\n",
    "\t\t\n",
    "\t\tplt.suptitle('Random Forest vs XGBoost Model Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.show()\n",
    "\t\t\n",
    "\t\t# Print comparison summary\n",
    "\t\tprint(\"\\n📊 MODEL COMPARISON SUMMARY:\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\t\tprint(f\"{'Position':<15} {'RF Test R²':<12} {'XGB Test R²':<12} {'RF Test MAE':<12} {'XGB Test MAE':<12}\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\t\tfor i, pos in enumerate(common_positions):\n",
    "\t\t\tprint(f\"{pos:<15} {rf_test_r2[i]:<12.3f} {xgb_test_r2[i]:<12.3f} {rf_test_mae[i]:<12.2f} {xgb_test_mae[i]:<12.2f}\")\n",
    "\t\t\n",
    "\t\t# Calculate averages and improvements\n",
    "\t\tavg_rf_r2 = np.mean(rf_test_r2)\n",
    "\t\tavg_xgb_r2 = np.mean(xgb_test_r2)\n",
    "\t\tavg_rf_mae = np.mean(rf_test_mae)\n",
    "\t\tavg_xgb_mae = np.mean(xgb_test_mae)\n",
    "\t\t\n",
    "\t\tprint(\"-\" * 80)\n",
    "\t\tprint(f\"{'Average':<15} {avg_rf_r2:<12.3f} {avg_xgb_r2:<12.3f} {avg_rf_mae:<12.2f} {avg_xgb_mae:<12.2f}\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\t\t\n",
    "\t\t# Calculate improvements\n",
    "\t\tr2_improvement = ((avg_xgb_r2 - avg_rf_r2) / avg_rf_r2) * 100\n",
    "\t\tmae_improvement = ((avg_rf_mae - avg_xgb_mae) / avg_rf_mae) * 100\n",
    "\t\t\n",
    "\t\tprint(f\"\\n🎯 XGBoost Performance vs Random Forest:\")\n",
    "\t\tprint(f\"   R² Score: {'↑' if r2_improvement > 0 else '↓'} {abs(r2_improvement):.1f}% {'better' if r2_improvement > 0 else 'worse'}\")\n",
    "\t\tprint(f\"   MAE:      {'↓' if mae_improvement > 0 else '↑'} {abs(mae_improvement):.1f}% {'better' if mae_improvement > 0 else 'worse'}\")\n",
    "\t\t\n",
    "\t\t# Determine overall winner\n",
    "\t\tif r2_improvement > 0 and mae_improvement > 0:\n",
    "\t\t\tprint(f\"\\n🏆 Winner: XGBoost (better on both metrics)\")\n",
    "\t\telif r2_improvement < 0 and mae_improvement < 0:\n",
    "\t\t\tprint(f\"\\n🏆 Winner: Random Forest (better on both metrics)\")\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"\\n🏆 Winner: Mixed results - depends on metric priority\")\n",
    "\n",
    "else:\n",
    "\tprint(\"⚠️ No model data available for plotting. Please ensure position_models and/or xgboost_models are defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ebe380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads509",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
