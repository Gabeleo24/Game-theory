{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install praw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Try to import optional packages\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    VADER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ùå VADER not installed. Please install: pip install vaderSentiment\")\n",
    "    VADER_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "    TEXTBLOB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  TextBlob not installed. Using VADER only.\")\n",
    "    TEXTBLOB_AVAILABLE = False\n",
    "\n",
    "class HistoricalRealMadridSentiment:\n",
    "    \"\"\"\n",
    "    Historical weekly sentiment analysis for Real Madrid fans from season start\n",
    "    Covers La Liga (August-May) and Champions League (September-June)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reddit_config):\n",
    "        \"\"\"Initialize Reddit API connection\"\"\"\n",
    "        \n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=reddit_config['client_id'],\n",
    "            client_secret=reddit_config['client_secret'],\n",
    "            user_agent=reddit_config['user_agent']\n",
    "        )\n",
    "        \n",
    "        # Initialize sentiment analyzers\n",
    "        if VADER_AVAILABLE:\n",
    "            self.vader_analyzer = SentimentIntensityAnalyzer()\n",
    "            print(\"‚úÖ VADER sentiment analyzer loaded\")\n",
    "        else:\n",
    "            self.vader_analyzer = None\n",
    "            print(\"‚ùå VADER not available\")\n",
    "        \n",
    "        # Real Madrid keywords\n",
    "        self.rm_keywords = [\n",
    "            'real madrid', 'madrid', 'bernabeu', 'bernab√©u', 'hala madrid',\n",
    "            'vinicius', 'bellingham', 'mbappe', 'modric', 'courtois',\n",
    "            'ancelotti', 'los blancos', 'madridista', 'santiago bernabeu',\n",
    "            'la liga', 'champions league', 'ucl', 'laliga'\n",
    "        ]\n",
    "        \n",
    "        # Season dates (2024-25 season)\n",
    "        self.season_start = datetime(2024, 8, 1)  # La Liga prep\n",
    "        self.season_end = datetime(2025, 6, 30)   # Champions League final\n",
    "        \n",
    "        # Competition periods\n",
    "        self.competitions = {\n",
    "            'la_liga': {\n",
    "                'start': datetime(2024, 8, 15),   # La Liga starts mid-August\n",
    "                'end': datetime(2025, 5, 25),     # La Liga ends late May\n",
    "                'keywords': ['la liga', 'laliga', 'league', 'primera']\n",
    "            },\n",
    "            'champions_league': {\n",
    "                'start': datetime(2024, 9, 15),   # Champions League starts mid-September\n",
    "                'end': datetime(2025, 6, 1),      # Champions League final\n",
    "                'keywords': ['champions league', 'ucl', 'european cup', 'champions']\n",
    "            },\n",
    "            'copa_del_rey': {\n",
    "                'start': datetime(2024, 11, 1),   # Copa del Rey starts November\n",
    "                'end': datetime(2025, 4, 30),     # Copa del Rey final\n",
    "                'keywords': ['copa del rey', 'copa', 'cup']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_season_weeks(self):\n",
    "        \"\"\"Generate all weeks from season start to current date\"\"\"\n",
    "        \n",
    "        weeks = []\n",
    "        current_date = self.season_start\n",
    "        today = datetime.now()\n",
    "        \n",
    "        week_number = 1\n",
    "        \n",
    "        while current_date < today and current_date < self.season_end:\n",
    "            week_end = current_date + timedelta(days=6)\n",
    "            \n",
    "            # Don't go beyond today\n",
    "            if week_end > today:\n",
    "                week_end = today\n",
    "            \n",
    "            # Determine active competitions for this week\n",
    "            active_competitions = []\n",
    "            for comp_name, comp_info in self.competitions.items():\n",
    "                if comp_info['start'] <= current_date <= comp_info['end']:\n",
    "                    active_competitions.append(comp_name)\n",
    "            \n",
    "            week_info = {\n",
    "                'week_number': week_number,\n",
    "                'start_date': current_date,\n",
    "                'end_date': week_end,\n",
    "                'active_competitions': active_competitions,\n",
    "                'is_complete': week_end < today\n",
    "            }\n",
    "            \n",
    "            weeks.append(week_info)\n",
    "            \n",
    "            # Move to next week\n",
    "            current_date = week_end + timedelta(days=1)\n",
    "            week_number += 1\n",
    "        \n",
    "        return weeks\n",
    "    \n",
    "    def collect_weekly_data(self, week_info, min_score=3, min_comments=2):\n",
    "        \"\"\"\n",
    "        Collect Reddit data for a specific week\n",
    "        \n",
    "        Args:\n",
    "            week_info (dict): Week information from get_season_weeks()\n",
    "            min_score (int): Minimum post score\n",
    "            min_comments (int): Minimum comments\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\nüìÖ Week {week_info['week_number']}: {week_info['start_date'].strftime('%Y-%m-%d')} to {week_info['end_date'].strftime('%Y-%m-%d')}\")\n",
    "        print(f\"üèÜ Active competitions: {', '.join(week_info['active_competitions']) if week_info['active_competitions'] else 'Off-season'}\")\n",
    "        \n",
    "        # Subreddits to search\n",
    "        subreddits = ['realmadrid', 'soccer', 'LaLiga']\n",
    "        all_posts = []\n",
    "        \n",
    "        # Convert to timestamps for Reddit API\n",
    "        start_timestamp = int(week_info['start_date'].timestamp())\n",
    "        end_timestamp = int(week_info['end_date'].timestamp())\n",
    "        \n",
    "        for subreddit_name in subreddits:\n",
    "            print(f\"   üìç Searching r/{subreddit_name}...\")\n",
    "            \n",
    "            try:\n",
    "                subreddit = self.reddit.subreddit(subreddit_name)\n",
    "                \n",
    "                # Search through different post types\n",
    "                post_sources = []\n",
    "                \n",
    "                # Get new posts (more likely to be from the specific week)\n",
    "                try:\n",
    "                    post_sources.extend(list(subreddit.new(limit=200)))\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Get hot posts\n",
    "                try:\n",
    "                    post_sources.extend(list(subreddit.hot(limit=100)))\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Filter posts by date and relevance\n",
    "                week_posts = []\n",
    "                \n",
    "                for post in post_sources:\n",
    "                    try:\n",
    "                        post_time = datetime.fromtimestamp(post.created_utc)\n",
    "                        \n",
    "                        # Check if post is in our week\n",
    "                        if not (week_info['start_date'] <= post_time <= week_info['end_date']):\n",
    "                            continue\n",
    "                        \n",
    "                        # Check reliability filters\n",
    "                        if post.score < min_score or post.num_comments < min_comments:\n",
    "                            continue\n",
    "                        \n",
    "                        # Check Real Madrid relevance\n",
    "                        post_text = (post.title + ' ' + post.selftext).lower()\n",
    "                        if not any(keyword in post_text for keyword in self.rm_keywords):\n",
    "                            continue\n",
    "                        \n",
    "                        # Determine competition context\n",
    "                        competition_context = self._detect_competition_context(\n",
    "                            post_text, week_info['active_competitions']\n",
    "                        )\n",
    "                        \n",
    "                        # Extract post data\n",
    "                        post_data = {\n",
    "                            'week_number': week_info['week_number'],\n",
    "                            'week_start': week_info['start_date'],\n",
    "                            'week_end': week_info['end_date'],\n",
    "                            'post_id': post.id,\n",
    "                            'subreddit': subreddit_name,\n",
    "                            'title': post.title,\n",
    "                            'selftext': post.selftext,\n",
    "                            'score': post.score,\n",
    "                            'upvote_ratio': post.upvote_ratio,\n",
    "                            'num_comments': post.num_comments,\n",
    "                            'created_utc': post_time,\n",
    "                            'author': str(post.author) if post.author else 'deleted',\n",
    "                            'permalink': f\"https://reddit.com{post.permalink}\",\n",
    "                            'competition_context': competition_context,\n",
    "                            'active_competitions': week_info['active_competitions'],\n",
    "                            'reliability_score': self._calculate_reliability_score(post)\n",
    "                        }\n",
    "                        \n",
    "                        # Get comments\n",
    "                        comments = self._extract_comments(post, limit=5)\n",
    "                        post_data['comments'] = comments\n",
    "                        post_data['comment_count'] = len(comments)\n",
    "                        \n",
    "                        week_posts.append(post_data)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"     ‚ö†Ô∏è Error processing post: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                all_posts.extend(week_posts)\n",
    "                print(f\"     ‚úÖ Found {len(week_posts)} relevant posts\")\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(0.2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå Error accessing r/{subreddit_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"üìä Week {week_info['week_number']} total: {len(all_posts)} posts\")\n",
    "        return all_posts\n",
    "    \n",
    "    def _detect_competition_context(self, text, active_competitions):\n",
    "        \"\"\"Detect which competition the post is about\"\"\"\n",
    "        \n",
    "        detected = []\n",
    "        \n",
    "        for comp_name in active_competitions:\n",
    "            comp_keywords = self.competitions[comp_name]['keywords']\n",
    "            if any(keyword in text for keyword in comp_keywords):\n",
    "                detected.append(comp_name)\n",
    "        \n",
    "        # If no specific competition detected, mark as general\n",
    "        if not detected and active_competitions:\n",
    "            return ['general']\n",
    "        elif not detected:\n",
    "            return ['off_season']\n",
    "        \n",
    "        return detected\n",
    "    \n",
    "    def _calculate_reliability_score(self, post):\n",
    "        \"\"\"Calculate reliability score based on Reddit metrics\"\"\"\n",
    "        \n",
    "        # Base score from upvotes\n",
    "        score_factor = min(post.score / 50, 1.0)  # Lower threshold for historical data\n",
    "        \n",
    "        # Upvote ratio (higher is better)\n",
    "        ratio_factor = post.upvote_ratio\n",
    "        \n",
    "        # Comment engagement\n",
    "        comment_factor = min(post.num_comments / 25, 1.0)  # Lower threshold\n",
    "        \n",
    "        # Author credibility (if available)\n",
    "        author_factor = 0.1 if post.author and hasattr(post.author, 'comment_karma') else 0\n",
    "        if author_factor and post.author.comment_karma > 500:  # Lower threshold\n",
    "            author_factor = 0.2\n",
    "        \n",
    "        reliability = (score_factor * 0.3 + \n",
    "                      ratio_factor * 0.4 + \n",
    "                      comment_factor * 0.2 + \n",
    "                      author_factor * 0.1)\n",
    "        \n",
    "        return round(reliability, 3)\n",
    "    \n",
    "    def _extract_comments(self, post, limit=5):\n",
    "        \"\"\"Extract top comments from a post\"\"\"\n",
    "        \n",
    "        comments = []\n",
    "        try:\n",
    "            post.comments.replace_more(limit=1)\n",
    "            \n",
    "            top_comments = sorted(post.comments, key=lambda x: x.score, reverse=True)\n",
    "            \n",
    "            for comment in top_comments[:limit]:\n",
    "                if hasattr(comment, 'body') and comment.score > 0:\n",
    "                    comment_data = {\n",
    "                        'body': comment.body,\n",
    "                        'score': comment.score,\n",
    "                        'created_utc': datetime.fromtimestamp(comment.created_utc)\n",
    "                    }\n",
    "                    comments.append(comment_data)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return comments\n",
    "    \n",
    "    def analyze_historical_sentiment(self, historical_data):\n",
    "        \"\"\"Analyze sentiment for all historical data\"\"\"\n",
    "        \n",
    "        if not VADER_AVAILABLE:\n",
    "            print(\"‚ùå Cannot analyze sentiment without VADER\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\nüî¨ Analyzing sentiment for {len(historical_data)} posts...\")\n",
    "        \n",
    "        analyzed_data = []\n",
    "        \n",
    "        for post in historical_data:\n",
    "            try:\n",
    "                # Combine title and text\n",
    "                full_text = post['title']\n",
    "                if post['selftext']:\n",
    "                    full_text += ' ' + post['selftext']\n",
    "                \n",
    "                # Clean text\n",
    "                clean_text = self._clean_text(full_text)\n",
    "                \n",
    "                if len(clean_text.split()) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # VADER sentiment\n",
    "                vader_scores = self.vader_analyzer.polarity_scores(clean_text)\n",
    "                \n",
    "                # TextBlob if available\n",
    "                textblob_polarity = 0\n",
    "                if TEXTBLOB_AVAILABLE:\n",
    "                    try:\n",
    "                        blob = TextBlob(clean_text)\n",
    "                        textblob_polarity = blob.sentiment.polarity\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # Combined sentiment\n",
    "                if TEXTBLOB_AVAILABLE and textblob_polarity != 0:\n",
    "                    combined_sentiment = (vader_scores['compound'] * 0.7) + (textblob_polarity * 0.3)\n",
    "                else:\n",
    "                    combined_sentiment = vader_scores['compound']\n",
    "                \n",
    "                # Get sentiment label\n",
    "                if combined_sentiment >= 0.1:\n",
    "                    sentiment_label = 'Positive'\n",
    "                elif combined_sentiment <= -0.1:\n",
    "                    sentiment_label = 'Negative'\n",
    "                else:\n",
    "                    sentiment_label = 'Neutral'\n",
    "                \n",
    "                # Create analysis\n",
    "                analysis = {\n",
    "                    **post,\n",
    "                    'clean_text': clean_text,\n",
    "                    'vader_compound': vader_scores['compound'],\n",
    "                    'vader_positive': vader_scores['pos'],\n",
    "                    'vader_negative': vader_scores['neg'],\n",
    "                    'vader_neutral': vader_scores['neu'],\n",
    "                    'textblob_polarity': textblob_polarity,\n",
    "                    'combined_sentiment': round(combined_sentiment, 3),\n",
    "                    'sentiment_label': sentiment_label,\n",
    "                    'overall_engagement': post['score'] + post['num_comments']\n",
    "                }\n",
    "                \n",
    "                analyzed_data.append(analysis)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Error analyzing post: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ Sentiment analysis complete: {len(analyzed_data)} posts\")\n",
    "        return analyzed_data\n",
    "    \n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Clean text for sentiment analysis\"\"\"\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "        \n",
    "        # Remove Reddit formatting\n",
    "        text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)\n",
    "        text = re.sub(r'\\/u\\/\\w+', '', text)\n",
    "        text = re.sub(r'\\/r\\/\\w+', '', text)\n",
    "        \n",
    "        # Remove special characters but keep Spanish accents\n",
    "        text = re.sub(r'[^\\w\\s\\u00C0-\\u024F\\u1E00-\\u1EFF]', ' ', text)\n",
    "        \n",
    "        # Clean whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def generate_historical_report(self, analyzed_data):\n",
    "        \"\"\"Generate comprehensive historical report\"\"\"\n",
    "        \n",
    "        if not analyzed_data:\n",
    "            print(\"‚ùå No data to analyze\")\n",
    "            return None\n",
    "        \n",
    "        df = pd.DataFrame(analyzed_data)\n",
    "        \n",
    "        print(f\"\\nüìä REAL MADRID HISTORICAL SENTIMENT REPORT\")\n",
    "        print(f\"=\" * 60)\n",
    "        \n",
    "        # Season overview\n",
    "        total_weeks = df['week_number'].nunique()\n",
    "        total_posts = len(df)\n",
    "        \n",
    "        season_start = df['week_start'].min().strftime('%Y-%m-%d')\n",
    "        latest_week = df['week_start'].max().strftime('%Y-%m-%d')\n",
    "        \n",
    "        print(f\"üèÜ SEASON OVERVIEW:\")\n",
    "        print(f\"   üìÖ Period: {season_start} to {latest_week}\")\n",
    "        print(f\"   üìä Total weeks: {total_weeks}\")\n",
    "        print(f\"   üìà Total posts: {total_posts}\")\n",
    "        print(f\"   üìä Average posts per week: {total_posts/total_weeks:.1f}\")\n",
    "        \n",
    "        # Weekly sentiment trends\n",
    "        weekly_stats = df.groupby('week_number').agg({\n",
    "            'combined_sentiment': 'mean',\n",
    "            'sentiment_label': lambda x: x.value_counts().index[0],\n",
    "            'overall_engagement': 'sum',\n",
    "            'post_id': 'count',\n",
    "            'week_start': 'first'\n",
    "        }).round(3)\n",
    "        \n",
    "        print(f\"\\nüìà WEEKLY SENTIMENT TRENDS:\")\n",
    "        print(f\"{'Week':<4} {'Date':<10} {'Sentiment':<9} {'Mood':<8} {'Posts':<5} {'Engagement':<10}\")\n",
    "        print(f\"-\" * 55)\n",
    "        \n",
    "        for week_num, row in weekly_stats.iterrows():\n",
    "            date_str = row['week_start'].strftime('%m/%d')\n",
    "            sentiment_str = f\"{row['combined_sentiment']:.3f}\"\n",
    "            mood_emoji = \"üòä\" if row['combined_sentiment'] > 0.1 else \"üòû\" if row['combined_sentiment'] < -0.1 else \"üòê\"\n",
    "            \n",
    "            print(f\"{week_num:<4} {date_str:<10} {sentiment_str:<9} {mood_emoji:<8} {row['post_id']:<5} {row['overall_engagement']:<10}\")\n",
    "        \n",
    "        # Competition-specific analysis\n",
    "        print(f\"\\nüèÜ COMPETITION-SPECIFIC SENTIMENT:\")\n",
    "        \n",
    "        # Flatten competition context for analysis\n",
    "        comp_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            for comp in row['competition_context']:\n",
    "                comp_entry = row.copy()\n",
    "                comp_entry['competition'] = comp\n",
    "                comp_data.append(comp_entry)\n",
    "        \n",
    "        if comp_data:\n",
    "            comp_df = pd.DataFrame(comp_data)\n",
    "            comp_sentiment = comp_df.groupby('competition').agg({\n",
    "                'combined_sentiment': 'mean',\n",
    "                'post_id': 'count'\n",
    "            }).round(3)\n",
    "            \n",
    "            for comp, row in comp_sentiment.iterrows():\n",
    "                mood = \"üòä\" if row['combined_sentiment'] > 0.1 else \"üòû\" if row['combined_sentiment'] < -0.1 else \"üòê\"\n",
    "                print(f\"   {comp.replace('_', ' ').title()}: {row['combined_sentiment']:.3f} {mood} ({row['post_id']} posts)\")\n",
    "        \n",
    "        # Overall season sentiment\n",
    "        overall_sentiment = df['combined_sentiment'].mean()\n",
    "        overall_mood = \"üòä POSITIVE SEASON\" if overall_sentiment > 0.1 else \"üòû CHALLENGING SEASON\" if overall_sentiment < -0.1 else \"üòê MIXED SEASON\"\n",
    "        \n",
    "        print(f\"\\nüéØ SEASON SUMMARY:\")\n",
    "        print(f\"   üìä Overall sentiment: {overall_sentiment:.3f}\")\n",
    "        print(f\"   üé≠ Season mood: {overall_mood}\")\n",
    "        \n",
    "        # Best and worst weeks\n",
    "        best_week = weekly_stats.loc[weekly_stats['combined_sentiment'].idxmax()]\n",
    "        worst_week = weekly_stats.loc[weekly_stats['combined_sentiment'].idxmin()]\n",
    "        \n",
    "        print(f\"   üìà Best week: Week {weekly_stats['combined_sentiment'].idxmax()} ({best_week['combined_sentiment']:.3f})\")\n",
    "        print(f\"   üìâ Worst week: Week {weekly_stats['combined_sentiment'].idxmin()} ({worst_week['combined_sentiment']:.3f})\")\n",
    "        \n",
    "        # Reliability assessment\n",
    "        avg_reliability = df['reliability_score'].mean()\n",
    "        print(f\"   üéØ Data reliability: {avg_reliability:.3f}\")\n",
    "        \n",
    "        # Create structured report\n",
    "        report = {\n",
    "            'season_overview': {\n",
    "                'start_date': season_start,\n",
    "                'latest_week': latest_week,\n",
    "                'total_weeks': total_weeks,\n",
    "                'total_posts': total_posts,\n",
    "                'avg_posts_per_week': total_posts/total_weeks\n",
    "            },\n",
    "            'overall_sentiment': overall_sentiment,\n",
    "            'season_mood': overall_mood,\n",
    "            'weekly_trends': weekly_stats.to_dict('index'),\n",
    "            'competition_sentiment': comp_sentiment.to_dict('index') if comp_data else {},\n",
    "            'best_week': {\n",
    "                'week': int(weekly_stats['combined_sentiment'].idxmax()),\n",
    "                'sentiment': float(best_week['combined_sentiment'])\n",
    "            },\n",
    "            'worst_week': {\n",
    "                'week': int(weekly_stats['combined_sentiment'].idxmin()),\n",
    "                'sentiment': float(worst_week['combined_sentiment'])\n",
    "            },\n",
    "            'reliability_score': avg_reliability\n",
    "        }\n",
    "        \n",
    "        return report, df\n",
    "    \n",
    "    def save_historical_data(self, analyzed_data, report=None, filename=None):\n",
    "        \"\"\"Save historical data and report\"\"\"\n",
    "        \n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f'real_madrid_historical_sentiment_{timestamp}'\n",
    "        \n",
    "        # Save detailed data\n",
    "        df = pd.DataFrame(analyzed_data)\n",
    "        df.to_csv(f'{filename}.csv', index=False, encoding='utf-8')\n",
    "        \n",
    "        # Save report if provided\n",
    "        if report:\n",
    "            with open(f'{filename}_report.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(report, f, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        print(f\"\\nüíæ Historical data saved:\")\n",
    "        print(f\"   üìä {filename}.csv ({len(df)} posts)\")\n",
    "        if report:\n",
    "            print(f\"   üìã {filename}_report.json\")\n",
    "    \n",
    "    def collect_full_season(self, save_weekly=True, max_weeks=None):\n",
    "        \"\"\"\n",
    "        Collect sentiment data for the entire season week by week\n",
    "        \n",
    "        Args:\n",
    "            save_weekly (bool): Save data after each week\n",
    "            max_weeks (int): Limit number of weeks (for testing)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"üèÜ COLLECTING FULL SEASON REAL MADRID SENTIMENT DATA\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Get all weeks\n",
    "        all_weeks = self.get_season_weeks()\n",
    "        \n",
    "        if max_weeks:\n",
    "            all_weeks = all_weeks[:max_weeks]\n",
    "            print(f\"‚ö†Ô∏è  Limited to first {max_weeks} weeks for testing\")\n",
    "        \n",
    "        print(f\"üìÖ Total weeks to process: {len(all_weeks)}\")\n",
    "        \n",
    "        # Collect data for each week\n",
    "        all_historical_data = []\n",
    "        \n",
    "        for i, week_info in enumerate(all_weeks, 1):\n",
    "            print(f\"\\nüîÑ Processing week {i}/{len(all_weeks)}...\")\n",
    "            \n",
    "            try:\n",
    "                week_data = self.collect_weekly_data(week_info)\n",
    "                \n",
    "                if week_data:\n",
    "                    # Analyze sentiment immediately\n",
    "                    week_analyzed = self.analyze_historical_sentiment(week_data)\n",
    "                    all_historical_data.extend(week_analyzed)\n",
    "                    \n",
    "                    # Save weekly if requested\n",
    "                    if save_weekly and week_analyzed:\n",
    "                        week_filename = f\"week_{week_info['week_number']:02d}_{week_info['start_date'].strftime('%Y%m%d')}\"\n",
    "                        self.save_historical_data(week_analyzed, filename=week_filename)\n",
    "                \n",
    "                # Rate limiting between weeks\n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing week {week_info['week_number']}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n‚úÖ COLLECTION COMPLETE!\")\n",
    "        print(f\"üìä Total posts collected: {len(all_historical_data)}\")\n",
    "        \n",
    "        if all_historical_data:\n",
    "            # Generate full season report\n",
    "            report, df = self.generate_historical_report(all_historical_data)\n",
    "            \n",
    "            # Save full season data\n",
    "            self.save_historical_data(all_historical_data, report, \"full_season_sentiment\")\n",
    "            \n",
    "            return all_historical_data, report\n",
    "        \n",
    "        return [], None\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    \"\"\"Main function to collect historical Real Madrid sentiment\"\"\"\n",
    "    \n",
    "    print(\"‚öΩ REAL MADRID HISTORICAL WEEKLY SENTIMENT ANALYZER\")\n",
    "    print(\"üèÜ From La Liga & Champions League Season Start\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Reddit API configuration\n",
    "    reddit_config = {\n",
    "        'client_id': 'your_client_id',\n",
    "        'client_secret': 'your_client_secret',\n",
    "        'user_agent': 'real_madrid_historical_sentiment_v1.0'\n",
    "    }\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = HistoricalRealMadridSentiment(reddit_config)\n",
    "    \n",
    "    # Show season overview\n",
    "    weeks = analyzer.get_season_weeks()\n",
    "    print(f\"\\nüìÖ SEASON OVERVIEW:\")\n",
    "    print(f\"   üèÜ Total weeks available: {len(weeks)}\")\n",
    "    print(f\"   üìÖ Season start: {analyzer.season_start.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   üìÖ Current week: Week {len(weeks)}\")\n",
    "    \n",
    "    # Ask user for collection scope\n",
    "    print(f\"\\nüîß COLLECTION OPTIONS:\")\n",
    "    print(f\"1. Full season (all {len(weeks)} weeks) - May take 1-2 hours\")\n",
    "    print(f\"2. Last 4 weeks (testing)\")\n",
    "    print(f\"3. Specific week range\")\n",
    "    \n",
    "    choice = input(\"Choose option (1-3): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        # Full season collection\n",
    "        print(f\"\\nüöÄ Starting full season collection...\")\n",
    "        historical_data, report = analyzer.collect_full_season(save_weekly=True)\n",
    "        \n",
    "    elif choice == \"2\":\n",
    "        # Test with last 4 weeks\n",
    "        print(f\"\\nüß™ Testing with last 4 weeks...\")\n",
    "        historical_data, report = analyzer.collect_full_season(save_weekly=False, max_weeks=4)\n",
    "        \n",
    "    elif choice == \"3\":\n",
    "        # Custom range\n",
    "        start_week = int(input(f\"Start week (1-{len(weeks)}): \"))\n",
    "        end_week = int(input(f\"End week ({start_week}-{len(weeks)}): \"))\n",
    "        \n",
    "        custom_weeks = weeks[start_week-1:end_week]\n",
    "        print(f\"\\nüéØ Processing weeks {start_week} to {end_week}...\")\n",
    "        \n",
    "        # Process custom range\n",
    "        all_data = []\n",
    "        for week_info in custom_weeks:\n",
    "            week_data = analyzer.collect_weekly_data(week_info)\n",
    "            if week_data:\n",
    "                analyzed = analyzer.analyze_historical_sentiment(week_data)\n",
    "                all_data.extend(analyzed)\n",
    "        \n",
    "        if all_data:\n",
    "            report, df = analyzer.generate_historical_report(all_data)\n",
    "            analyzer.save_historical_data(all_data, report)\n",
    "        \n",
    "        historical_data = all_data\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Invalid choice\")\n",
    "        return\n",
    "    \n",
    "    if historical_data:\n",
    "        print(f\"\\nüéØ ANALYSIS COMPLETE!\")\n",
    "        print(f\"üìä Total posts analyzed: {len(historical_data)}\")\n",
    "        if report:\n",
    "            print(f\"üìà Season sentiment: {report['overall_sentiment']:.3f}\")\n",
    "            print(f\"üé≠ Season mood: {report['season_mood']}\")\n",
    "    else:\n",
    "        print(\"‚ùå No data collected\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üìã SETUP FOR HISTORICAL ANALYSIS:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"1. Install: pip install praw pandas vaderSentiment\")\n",
    "    print(\"2. Get Reddit API credentials\")\n",
    "    print(\"3. This will collect data from August 2024 to present\")\n",
    "    print(\"4. Full season collection may take 1-2 hours\")\n",
    "    print(\"5. Data is saved weekly for backup\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Check if user wants to run\n",
    "    run_choice = input(\"Ready to start historical collection? (y/n): \").strip().lower()\n",
    "    if run_choice == 'y':\n",
    "        main()\n",
    "    else:\n",
    "        print(\"Setup your Reddit credentials first, then run the script!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
